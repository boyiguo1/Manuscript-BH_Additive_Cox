@book{Wood2017,
abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book's R data package gamair, to enable use as a course text or for self-study.},
author = {Wood, Simon N.},
booktitle = {Generalized Additive Models: An Introduction with R, Second Edition},
doi = {10.1201/9781315370279},
isbn = {9781498728348},
title = {{Generalized additive models: An introduction with R, second edition}},
year = {2017}
}

@article{Nychka1988,
abstract = {The frequency properties of Wahba's Bayesian confidence intervals for smoothing splines are investigated by a large-sample approximation and by a simulation study. When the coverage probabilities for these pointwise confidence intervals are averaged across the observation points, the average coverage probability (ACP) should be close to the nominal level. From a frequency point of view, this agreement occurs because the average posterior variance for the spline is similar to a consistent estimate of the average squared error and because the average squared bias is a modest fraction of the total average squared error. These properties are independent of the Bayesian assumptions used to derive this confidence procedure, and they explain why the ACP is accurate for functions that are much smoother than the sample paths prescribed by the prior. This analysis accounts for the choice of the smoothing parameter (bandwidth) using cross-validation. In the case of natural splines an adaptive method for avoiding boundary effects is considered. The main disadvantage of this approach is that these confidence intervals are only valid in an average sense and may not be reliable if only evaluated at peaks or troughs in the estimate. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
annote = {LImitation of Splines: hard to specifying measures of estimate accuracy},
author = {Nychka, Douglas},
doi = {10.1080/01621459.1988.10478711},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/2290146.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Boundary effects,Cross-validation,Nonparametric regression,Smoothing parameter},
number = {404},
pages = {1134--1143},
title = {{Bayesian confidence intervals for smoothing splines}},
volume = {83},
year = {1988}
}

@article{Wahba1983,
author = {Wahba, Grace},
doi = {10.1111/j.2517-6161.1983.tb01239.x},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/2345632.pdf:pdf},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {sep},
number = {1},
pages = {133--150},
title = {{Bayesian “Confidence Intervals” for the Cross-Validated Smoothing Spline}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1983.tb01239.x},
volume = {45},
year = {1983}
}

@article{Silverman1985,
author = {Silverman, B. W.},
doi = {10.1111/j.2517-6161.1985.tb01327.x},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/Silverman-JRSSB85.pdf:pdf},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {sep},
number = {1},
pages = {1--21},
title = {{Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1985.tb01327.x},
volume = {47},
year = {1985}
}

@article{Kimeldorf1970,
abstract = {The asymptotic behaviour of the residual life time at time t is investigated (for t rightarrow infty). We derive weak limit laws and their domains of attraction and treat rates of convergence and moment convergence. The presentation exploits the close similarity with extreme value theory.},
author = {Kimeldorf, George S. and Wahba, Grace},
doi = {10.1214/aoms/1177697089},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/euclid.aoms.1177697089.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {apr},
number = {2},
pages = {495--502},
title = {{A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines}},
url = {http://projecteuclid.org/euclid.aop/1176996548 http://projecteuclid.org/euclid.aoms/1177697089},
volume = {41},
year = {1970}
}

@article{Wood2006,
abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods. {\textcopyright} 2006, The International Biometric Society.},
author = {Wood, Simon N.},
doi = {10.1111/j.1541-0420.2006.00574.x},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/j.1541-0420.2006.00574.x.pdf:pdf},
issn = {15410420},
journal = {Biometrics},
keywords = {Computationally efficient,Generalized additive mixed model (GAMM),Mixed effect variable coefficient model,Multiple penalties,Penalized regression,SS-ANOVA,Scale invariant,Smooth interaction,Smoothing penalty,Spline,Tensor product smooth},
number = {4},
pages = {1025--1036},
pmid = {17156276},
title = {{Low-rank scale-invariant tensor product smooths for generalized additive mixed models}},
volume = {62},
year = {2006}
}

@article{Yi2012,
abstract = {Genetic and other scientific studies routinely generate very many predictor variables, which can be naturally grouped, with predictors in the same groups being highly correlated. It is desirable to incorporate the hierarchical structure of the predictor variables into generalized linear models for simultaneous variable selection and coefficient estimation. We propose two prior distributions: hierarchical Cauchy and double-exponential distributions, on coefficients in generalized linear models. The hierarchical priors include both variable-specific and group-specific tuning parameters, thereby not only adopting different shrinkage for different coefficients and different groups but also providing a way to pool the information within groups. We fit generalized linear models with the proposed hierarchical priors by incorporating flexible expectation-maximization (EM) algorithms into the standard iteratively weighted least squares as implemented in the general statistical package R. The methods are illustrated with data from an experiment to identify genetic polymorphisms for survival of mice following infection with Listeria monocytogenes. The performance of the proposed procedures is further assessed via simulation studies. The methods are implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/). {\textcopyright} 2012 De Gruyter. All rights reserved.},
author = {Yi, Nengjun and Ma, Shuangge},
doi = {10.1515/1544-6115.1803},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yi, Ma - 2012 - Hierarchical Shrinkage Priors and Model Fitting for High-dimensional Generalized Linear Models.pdf:pdf},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Adaptive lasso,Bayesian inference,Generalized linear model,Genetic polymorphisms,Grouped variables,Hierarchical model,High-dimensional data,Shrinkage prior},
month = {jan},
number = {6},
pmid = {23192052},
title = {{Hierarchical Shrinkage Priors and Model Fitting for High-dimensional Generalized Linear Models}},
url = {https://www.degruyter.com/view/j/sagmb.2012.11.issue-6/1544-6115.1803/1544-6115.1803.xml},
volume = {11},
year = {2012}
}

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media}
}


@article{Bennette2012,
abstract = {Background: Quantiles are a staple of epidemiologic research: in contemporary epidemiologic practice, continuous variables are typically categorized into tertiles, quartiles and quintiles as a means to illustrate the relationship between a continuous exposure and a binary outcome. Discussion. In this paper we argue that this approach is highly problematic and present several potential alternatives. We also discuss the perceived drawbacks of these newer statistical methods and the possible reasons for their slow adoption by epidemiologists. Summary. The use of quantiles is often inadequate for epidemiologic research with continuous variables. {\textcopyright} 2012 Bennette and Vickers; licensee BioMed Central Ltd.},
author = {Bennette, Caroline and Vickers, Andrew},
doi = {10.1186/1471-2288-12-21},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/Bennette-Vickers2012{\_}Article{\_}AgainstQuantilesCategorization.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
pmid = {22375553},
title = {{Against quantiles: Categorization of continuous variables in epidemiologic research, and its discontents}},
volume = {12},
year = {2012}
}


@article{Greenland1995,
author = {Greenland, Sander},
doi = {10.1097/00001648-199507000-00025},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/3702100.pdf:pdf},
issn = {15315487},
journal = {Epidemiology},
number = {4},
pages = {450},
pmid = {7548361},
title = {{Avoiding power loss associated with categorization and ordinal scores in dose-response and trend analysis}},
volume = {6},
year = {1995}
}

@book{James2013,
address = {New York, NY},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1007/978-1-4614-7138-7},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/An{\_}Introduction{\_}to{\_}Statistical{\_}Learning.pdf:pdf},
isbn = {978-1-4614-7137-0},
issn = {1431-875X},
pmid = {10911016},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{An Introduction to Statistical Learning}},
url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
volume = {103},
year = {2013}
}


@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre- lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna- tional conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/Breiman2001{\_}Article{\_}RandomForests.pdf:pdf},
isbn = {9783110941975},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
number = {45},
pages = {5--32},
title = {{Random Forests}},
url = {https://link.springer.com/article/10.1023/A:1010933404324},
year = {2001}
}

@article{BreimanTwoCulture,
abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools. {\textcopyright} 2001 Institute of Mathematical Statistics.},
author = {Breiman, Leo},
doi = {10.1214/ss/1009213726},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/1009213726.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {199--215},
title = {{Statistical modeling: The two cultures}},
volume = {16},
year = {2001}
}


@article{Cleveland1979,
abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (xi, yi), i = 1, {\ldots}, n, in which the fitted value at zkis the value of a polynomial fit to the data using weighted least squares, where the weight for (xi, yi) is large if xiis close to xkand small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology. {\textcopyright} 1979, Taylor {\&} Francis Group, LLC.},
author = {Cleveland, William S.},
doi = {10.1080/01621459.1979.10481038},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2286407.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Graphics,Nonparametric regression,Robust estimation,Scatterplots,Smoothing},
number = {368},
pages = {829--836},
title = {{Robust locally weighted regression and smoothing scatterplots}},
volume = {74},
year = {1979}
}

@article{Friedman1981,
abstract = {A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation. {\textcopyright} 1981 Taylor {\&} Francis Group, LLC.},
author = {Friedman, Jerome H. and Stuetzle, Werner},
doi = {10.1080/01621459.1981.10477729},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2287576.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Nonparametric regression,Projection pursuit,Smoothing,Surface approximation},
number = {376},
pages = {817--823},
title = {{Projection pursuit regression}},
volume = {76},
year = {1981}
}

@article{Hastie1987,
abstract = {Generalized additive models have the form $\eta$(x) = $\alpha$ + $\sigma$ fj(xj), where $\eta$ might be the regression function in a multiple regression or the logistic transformation of the posterior probability Pr(y = 1 x) in a logistic regression. In fact, these models generalize the whole family of generalized linear models $\eta$(x) = $\beta$′x, where $\eta$(x) = g($\mu$(x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions fj(xj) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximate hypothesis tests. The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
author = {Hastie, Trevor and Tibshirani, Robert},
doi = {10.1080/01621459.1987.10478440},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2289439.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Generalized linear model,Logistic regression,Nonparametric regression,Smooth},
number = {398},
pages = {371--386},
title = {{Generalized additive models: Some applications}},
volume = {82},
year = {1987}
}


@article{Nelder1972,
author = {Nelder, J. A. and Wedderburn, R. W. M.},
doi = {10.2307/2344614},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nelder, Wedderburn - 1972 - Generalized Linear Models.pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
number = {3},
pages = {370},
title = {{Generalized Linear Models}},
url = {https://www.jstor.org/stable/10.2307/2344614?origin=crossref},
volume = {135},
year = {1972}
}

@article{Ruppert1999,
annote = {Ruppert {\&} Carroll (1999) gave an overview of an important regression tool, penalized regression spline, and highlighted a particular family of penalty, nonquadratic penalty. Particullarly, they mentioned the benefits of using nonquadratic penalty when A drastic jump exists, which is common in two-dimensional image analysis.

In the discussion section, the authors discussed the future diretion, where semiparametric models, i.e. including splines and non-spline variables, and bayesian inference.

The authors discussed that Bayesian inference could help on different probelms, such as tests of submodels and confidence intervals and bands for univariate curves. Meanwhile, the authors mentioned that Bayesian aanalysis is approriate to regression splines becuase of the finiately dimensional space, which contrasts to smoothing splines. The authors recommended non-gaussian priors, e.g. double exponential priors.},
author = {Ruppert, David and Carroll, Raymond J.},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruppert, Carroll - 1999 - Penalized Regression Splines.pdf:pdf},
journal = {Cornell University Operations Research and Industrial Engineering.},
title = {{Penalized Regression Splines}},
year = {1999}
}


@article{OSullivan1988,
abstract = {The computation of fully automated or data-driven penalized likelihood estimators of logarithmic densities and hazards is accomplished using B-spline approximants. The banded structure induced by B-splines gives rise to very fast and statistically effective algorithms. Automation is achieved using one-step Newton-Raphson cross-validation scores which are simplified to forms reminiscent of AIC criteria. The cost of computing these scores is linear in the number of data points. Approximate Bayesian confidence intervals are also derived and illustrated with some examples. Such intervals provide a valuable aid to the interpretation of estimated curves. The small sample performance (bias and variability) of the estimators is investigated by a simulation study.},
author = {O'Sullivan, Finbarr},
doi = {10.1137/0909024},
issn = {0196-5204},
journal = {SIAM Journal on Scientific and Statistical Computing},
title = {{Fast Computation of Fully Automated Log-Density and Log-Hazard Estimators}},
year = {1988}
}

@article{OSullivan1986,
abstract = {Ill-posed inverse problems arise in many branches of science and engineering. In the typical situation one is interested in recovering a whole function given a finite number of noisy measurements on functionals. Performance characteristics of an inversion algorithm are studied via the mean square error which is decomposed into bias and variability. Variability calculations are often straightforward, but useful bias measures are more difficult to obtain. An appropriate definition of what geophysicists call the Backus-Gilbert averaging kernel leads to a natural way of measuring bias characteristics. Moreover, the ideas give rise to some important experimental design criteria. It can be shown that the optimal inversion algorithms are methods of regularization procedures, but to completely specify these algorithms the signal to noise ratio must be supplied. Statistical approaches to the empirical determination of the signal to noise ratio are discussed; cross-validation and unbiased risk methods are reviewed; and some extensions, which seem particularly appropriate in the inverse problem context, are indicated. Linear and nonlinear examples from medicine, meteorology, and geophysics are used for illustration. {\textcopyright} 1986, Institute of Mathematical Statistics. All Rights Reserved.},
author = {O'Sullivan, Finbarr},
doi = {10.1214/ss/1177013525},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/2245801.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Averaging Kernel,B-splines,Cross-validation,Experimental design,Mean square error,Reservoir engineering,Satellite meteorology,Stereology},
number = {4},
pages = {502--518},
title = {{A statistical perspective on III-posed inverse problems}},
volume = {1},
year = {1986}
}

@article{Eilers1996,
annote = {P-spline was originaly proposed by Eilers and Marx 1996, where they considered a combination of B-splines and difference penalities. The popularity of P-spline thanks to many of its merits: 1) easily extended from the generalized linear models, and 2) its estimation process is relatively inexpensive. Due to the possible vulnerabillity of overfitting, penalties was used to restrict the faexibility of the fitted curve (O' Sullivan 1986, 1988). Contrary to O'sullivan who used the second dirvative of the fitted curve as the penalty. Eilers and Marx generalized the penalty by using a difference penalty on the coefficients of adjacent B-splines. P-spline doesn't suffer from boundry effect unlike a lot of other smoothers.},
author = {Eilers, Paul H C and Marx, Brian D},
doi = {10.1214/ss/1038425655},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eilers, Marx - 1996 - Flexible smoothing with B -splines and penalties.pdf:pdf},
isbn = {0226808068},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,density estimation,generalized linear models,non-,parametric models,smoothing,splines},
month = {may},
number = {2},
pages = {89--121},
title = {{Flexible smoothing with B -splines and penalties}},
url = {http://projecteuclid.org/euclid.ss/1038425655},
volume = {11},
year = {1996}
}

@article{Ibrahim2001,
abstract = {We propose methods for Bayesian inference for a new class of semiparametric survival models with a cure fraction. Specifically, we propose a semiparametric cure rate model with a smoothing parameter that controls the degree of parametricity in the right tail of the survival distribution. We show that such a parameter is crucial for these kinds of models and can have an impact on the posterior estimates. Several novel properties of the proposed model are derived. In addition, we propose a class of improper noninformative priors based on this model and examine the properties of the implied posterior. Also, a class of informative priors based on historical data is proposed and its theoretical properties are investigated. A case study involving a melanoma clinical trial is discussed in detail to demonstrate the proposed methodology.},
author = {Ibrahim, Joseph G. and Chen, Ming Hui and Sinha, Debajyoti},
doi = {10.1111/j.0006-341X.2001.00383.x},
issn = {0006341X},
journal = {Biometrics},
keywords = {Cure rate model,Gibbs sampling,Historical data,Latent variables,Piecewise exponential,Posterior distribution,Semiparametric model,Smoothing parameter},
pmid = {11414560},
title = {{Bayesian semiparametric models for survival data with a cure fraction}},
year = {2001}
}
@book{Klein2003,
abstract = {Applied statisticians in many fields must frequently analyze time to event data. While the statistical tools presented in this book are applicable to data from medicine, biology, public health, epidemiology, engineering, economics, and demography, the focus here is on applications of the techniques to biology and medicine. The analysis of survival experiments is complicated by issues of censoring, where an individual{\&}apos;s life length is known to occur only in a certain period of time, and by truncation, where individuals enter the study only if they survive a sufficient length of time or individuals are included in the study only if the event has occurred by a given date. The use of counting process methodology has allowed for substantial advances in the statistical theory to account for censoring and truncation in survival experiments. This book makes these complex methods more accessible to applied researchers without an advanced mathematical background. The authors present the essence of these techniques, as well as classical techniques not based on counting processes, and apply them to data. Practical suggestions for implementing the various methods are set off in a series of Practical Notes at the end of each section. Technical details of the derivation of the techniques are sketched in a series of Technical Notes. This book will be useful for investigators who need to analyze censored or truncated life time data, and as a textbook for a graduate course in survival analysis. The prerequisite is a standard course in statistical methodology.},
address = {New York, NY},
author = {Klein, John P. and Moeschberger, Melvin L.},
booktitle = {Pharmaceutical Statistics},
doi = {10.1007/b97377},
isbn = {978-0-387-95399-1},
issn = {1539-1604},
publisher = {Springer New York},
series = {Statistics for Biology and Health},
title = {{Survival Analysis}},
url = {http://link.springer.com/10.1007/b97377},
year = {2003}
}

@article{Efron1977,
abstract = {D.R. Cox has suggested a simple method for the regression analysis of censored data. We carry out an information calculation which shows that Cox's method has full asymptotic efficiency under conditions which are likely to be satisfied in many realistic situations. The connection of Cox's method with the Kaplan-Meier estimator of a survival curve is made explicit. {\textcopyright} 1977, Taylor {\&} Francis Group, LLC.},
author = {Efron, Bradley},
doi = {10.1080/01621459.1977.10480613},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Censored data,Cox likelihood,Survival curves},
title = {{The efficiency of cox's likelihood function for censored data}},
year = {1977}
}

@article{Steenland2004,
abstract = {Dose-response modeling in occupational epidemiology is usually motivated by questions of causal inference (eg, is there a monotonic increase of risk with increasing exposure?) or risk assessment (eg, how much excess risk exists at any given level of exposure?). We focus on several approaches to dose-response in occupational cohort studies. Categorical analyses are useful for detecting the shape of dose-response. However, they depend on the number and location of cutpoints and result in step functions rather than smooth curves. Restricted cubic splines and penalized splines are useful parametric techniques that provide smooth curves. Although splines can complement categorical analyses, they do not provide interpretable parameters. The shapes of these curves will depend on the degree of "smoothing" chosen by the analyst. We recommend combining categorical analyses and some type of smoother, with the goal of developing a reasonably simple parametric model. A simple parametric model should serve as the goal of dose-response analyses because (1) most "true" exposure response curves in nature may be reasonably simple, (2) a simple parametric model is easily communicated and used by others, and (3) a simple parametric model is the best tool for risk assessors and regulators seeking to estimate individual excess risks per unit of exposure. We discuss these issues and others, including whether the best model is always the one that fits the best, reasons to prefer a linear model for risk in the low-exposure region when conducting risk assessment, and common methods of calculating excess lifetime risk at a given exposure from epidemiologic results (eg, from rate ratios). Points are illustrated using data from a study of dioxin and cancer. Copyright {\textcopyright} 2003 by Lippincott Williams {\&} Wilkins.},
annote = {Gread review article. Have a lot of epidemology perspective on using spline},
author = {Steenland, Kyle and Deddens, James A.},
doi = {10.1097/01.ede.0000100287.45004.e7},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steenland, Deddens - 2004 - A practical guide to dose-response analyses and risk assessment in occupational epidemiology(2).pdf:pdf},
issn = {10443983},
journal = {Epidemiology},
number = {1},
pages = {63--70},
pmid = {14712148},
title = {{A practical guide to dose-response analyses and risk assessment in occupational epidemiology}},
volume = {15},
year = {2004}
}

@article{Govindarajulu2009,
abstract = {We examined the behavior of alternative smoothing methods for modeling environmental epidemiology data. Model fit can only be examined when the true exposure-response curve is known and so we used simulation studies to examine the performance of penalized splines (P-splines), restricted cubic splines (RCS), natural splines (NS), and fractional polynomials (FP). Survival data were generated under six plausible exposure-response scenarios with a right skewed exposure distribution, typical of environmental exposures. Cox models with each spline or FP were fit to simulated datasets. The best models, e.g. degrees of freedom, were selected using default criteria for each method. The root mean-square error (rMSE) and area difference were computed to assess model fit and bias (difference between the observed and true curves). The test for linearity was a measure of sensitivity and the test of the null was an assessment of statistical power. No one method performed best according to all four measures of performance, however, all methods performed reasonably well. The model fit was best for P-splines for almost all true positive scenarios, although fractional polynomials and RCS were least biased, on average. Copyright {\textcopyright}2009 The Berkeley Electronic Press. All rights reserved.},
author = {Govindarajulu, Usha S. and Malloy, Elizabeth J. and Ganguli, Bhaswati and Spiegelman, Donna and Eisen, Ellen A.},
doi = {10.2202/1557-4679.1104},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Govindarajulu et al. - 2009 - The comparison of alternative smoothing methods for fitting non-linear exposure-response relationships wit.pdf:pdf},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Cox model,Fractional polynomial,Natural spline,Penalized spline,Restricted cubic spline,Simulation},
number = {1},
pmid = {20231865},
title = {{The comparison of alternative smoothing methods for fitting non-linear exposure-response relationships with Cox models in a simulation study}},
volume = {5},
year = {2009}
}
@article{Govindarajulu2007,
abstract = {The additive hazards model is one of the most commonly used regression models in the analysis of failure time data and many methods have been developed for its inference under various situations. This paper discusses the situation where one faces current status data and also there exists informative censoring or when the failure time of interest and the observation process are correlated. Several authors have discussed the problem and in particular, Zhang et al. (2005) and Zhao et al. (2015) proposed an estimating equationbased approach and a copula model-based method, respectively. However, the former may not be efficient and the latter needs some restrictive assumptions. To address these, we propose a sieve maximum likelihood estimation approach that can be more efficient and also does not require the assumption above. For the implementation of the method, an EM algorithm is developed and the asymptotic properties of the resulting estimators are established. The numerical results suggest that the proposed method works well in practical situations and an application is provided.},
author = {Govindarajulu, Usha S. and Spiegelman, Donna and Thurston, Sally W. and Ganguli, Bhaswati and Eisen, Ellen A.},
doi = {10.1002/sim.2848},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Govindarajulu et al. - 2007 - Comparing smoothing techniques in Cox models for exposure–response relationships.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {publication bias,selection bias,selection model,sensitivity analysis,unpublished studies},
month = {sep},
number = {20},
pages = {3735--3752},
title = {{Comparing smoothing techniques in Cox models for exposure–response relationships}},
url = {http://doi.wiley.com/10.1002/sim.2848},
volume = {26},
year = {2007}
}

@book{Buhlmann2011,
abstract = {Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and ...},
address = {Berlin, Heidelberg},
author = {B{\"{u}}hlmann, Peter and van de Geer, Sara},
booktitle = {Journal of Applied Statistics},
doi = {10.1007/978-3-642-20192-9},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{u}}hlmann, van de Geer - 2011 - Statistics for High-Dimensional Data(2).pdf:pdf},
isbn = {978-3-642-20191-2},
issn = {0266-4763},
number = {10},
pages = {2308--2309},
publisher = {Springer Berlin Heidelberg},
series = {Springer Series in Statistics},
title = {{Statistics for High-Dimensional Data}},
url = {http://link.springer.com/10.1007/978-3-642-20192-9},
volume = {39},
year = {2011}
}

@article{Berry2002,
abstract = {In the presence of covariate measurement error, estimating a regression function nonparametrically is extremely difficult, the problem being related to deconvolution. Various frequentist approaches exist for this problem, but to date there has been no Bayesian treatment. In this article we describe Bayesian approaches to modeling a flexible regression function when the predictor Variable is measured with error. The regression function is modeled with smoothing splines and regression P-splines. Two methods are described for exploration of the posterior. The first, called the iterative conditional modes (ICM), is only partially Bayesian. ICM uses a componentwise maximization routine to find the mode of the posterior. It also serves to create starting values for the second method, which is fully Bayesian and uses Markov chain Monte Carlo (MCMC) techniques to generate observations from the joint posterior distribution. Use of the MCMC approach has the advantage that interval estimates that directly model and adjust for the measurement error are easily calculated. We provide simulations with several nonlinear regression functions and provide an illustrative example. Our simulations indicate that the frequentist mean squared error properties of the fully Bayesian method are better than those of ICM and also of previously proposed frequentist methods, at least in the examples that we have studied.},
annote = {In brevity, smoothing spline is a special case of P-splines according to @berry2002bayesian.},
author = {Berry, S. M. and Carroll, R. J. and Ruppert, D.},
doi = {10.1198/016214502753479301},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berry, Carroll, Ruppert - 2002 - Bayesian smoothing and regression splines for measurement error problems.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bayesian methods,Efficiency,Errors in variables,Functional method,Generalized linear models,Kernel regression,Measurement error,Nonparametric regression,P-splines,Regression splines,SIMEX method,Smoothing splines,Structural modeling},
number = {457},
pages = {160--169},
title = {{Bayesian smoothing and regression splines for measurement error problems}},
volume = {97},
year = {2002}
}
@article{Lang2004,
abstract = {P-splines are an attractive approach for modeling nonlinear smooth effects of covariates within the additive and varying coefficient models framework. In this article, we first develop a Bayesian version for P-splines and generalize in a second step the approach in various ways. First, the assumption of constant smoothing parameters can be replaced by allowing the smoothing parameters to be locally adaptive. This is particularly useful in situations with changing curvature of the underlying smooth function or with highly oscillating functions. In a second extension, one-dimensional P-splines are generalized to two-dimensional surface fitting for modeling interactions between metrical covariates. In a last step, the approach is extended to situations with spatially correlated responses allowing the estimation of geoadditive models. Inference is fully Bayesian and uses recent MCMC techniques for drawing random samples from the posterior. In a couple of simulation studies the performance of Bayesian P-splines is studied and compared to other approaches in the literature. We illustrate the approach by two complex application on rents for flats in Munich and on human brain mapping.},
annote = {Lang and Brezger(2004) introduced Bayesian Penalized Splines, where they introduced an locally adaptive smoothing by incorprating Gaussian Random Walk piror. Contrast to a global smoothing parameter setting, the locally adaptive smoothing setup performs better when the underlying data generating mechanism are highly oscilating. The estimation of the parameters was achieved by MCMC techinique, which is undoubtfully time-consuming.The authros also extended the additive modol to a mixed model with application to geo-data analysis.

In the real data anaysis, they showed improvement with comparison to frequentist methods.

In the conclusion section, the author discussed that a freqeuntist approach would not work as good when there are many unknown functions. 

Some of the open problems are expanding the gaussian model to generalized model, as well as improve the computation sppeed.},
author = {Lang, Stefan and Brezger, Andreas},
doi = {10.1198/1061860043010},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lang, Brezger - 2004 - Bayesian P-Splines(2).pdf:pdf},
isbn = {1061860043},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Geoadditive models,Locally adaptive smoothing parameters,MCMC,Surface fitting,Varying coefficient models},
month = {mar},
number = {1},
pages = {183--212},
title = {{Bayesian P-Splines}},
url = {http://www.tandfonline.com/doi/abs/10.1198/1061860043010},
volume = {13},
year = {2004}
}

@article{Cox1972,
abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
author = {Cox, David R.},
doi = {10.1111/j.2517-6161.1972.tb00899.x},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/2985181.pdf:pdf},
isbn = {00359246},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {age-specific failure rate,hazard function,life table,product},
month = {jan},
number = {2},
pages = {187--202},
pmid = {2985181},
title = {{Regression Models and Life-Tables}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1972.tb00899.x},
volume = {34},
year = {1972}
}

@article{Gray1994,
abstract = {This paper examines a method for testing hypotheses on covariate effects in a proportional hazards model, and also on how effects change over time in regression analysis of survival data. The technique used is very general and can be applied to testing many other aspects of parametric and semiparametric models. The basic idea is to formulate a flexible parametric alternative using fixed knot splines, together with a penalty function that penalizes noisy alternatives more than smooth ones, to focus the power of the tests toward smooth alternatives. The test statistics are the analogs of ordinary likelihood-based statistics, only computed from a penalized likelihood formed by subtracting the penalty function from the ordinary log-likelihood. Large-sample approximations to the distributions are found when the number of knots is held fixed as the sample size increases. Numerical results suggest these approximations may be adequate with moderate sized samples.},
author = {Gray, Robert J.},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/Spline-Based Tests in Survival Analysis.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
month = {sep},
number = {3},
pages = {640--52},
pmid = {7981391},
title = {{Spline-based tests in survival analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7981391},
volume = {50},
year = {1994}
}
@article{Gray1992,
abstract = {In this article some flexible methods for modeling censored survival data using splines are applied to the problem of modeling the time to recurrence of breast cancer patients. The basic idea is to use fixed knot splines with a fairly modest number of knots to model aspects of the data, and then to use penalized partial likelihood to estimate the parameters of the model. Test statistics are proposed which are analogs of those used in traditional likelihood analysis, and approximations to the distributions of these statistics are suggested. In an analysis of a large data set taken from clinical trials conducted by the Eastern Cooperative Oncology Group, these methods are seen to give useful insight into how prognosis varies as a function of continuous covariates, and also into how covariate effects change with follow-up time. {\textcopyright} 1992 Taylor {\&} Francis Group, LLC.},
author = {Gray, Robert J.},
doi = {10.1080/01621459.1992.10476248},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/2290630.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Breast cancer,Censored data,Penalized likelihood,Proportional hazards,Splines},
month = {dec},
number = {420},
pages = {942--951},
title = {{Flexible Methods for Analyzing Survival Data Using Splines, with Applications to Breast Cancer Prognosis}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10476248},
volume = {87},
year = {1992}
}
@article{Sleeper1990,
abstract = {The Cox proportional hazards model restricts the log hazard ratio to be linear in the covariates. A smooth nonlinear covariate effect may go undetected in this model but can be well approximated by a spline function. A survival model based on data from a clinical trial of primary biliary cirrhosis is developed using regression splines, and the resulting log hazard ratio estimates are compared with those from nonparametric methods. We remove the linear restriction on the log hazard ratio by transforming a continuous covariate into a vector of fixed knot basis splines (B-splines). B-splines are known to produce better-conditioned systems of equations than the truncated power basis when used as interpolants, and show similar behavior when fitting proportional hazards models. We describe the procedures for, and the issues arising in, the estimation and the testing of the B-spline coefficients. Although inference is not well developed for some nonparametric methods that estimate covariate effects, the asymptotic theory for Cox model B-spline estimates is relatively straightforward. An S function for fitting B-splines in Cox regression models is available. {\textcopyright} 1990 Taylor {\&} Francis Group, LLC.},
author = {Sleeper, Lynn A. and Harrington, David P.},
doi = {10.1080/01621459.1990.10474965},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/2289591.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {B-spline,Proportional hazards,Relative risk,Spline approximation,Survival analysis},
month = {dec},
number = {412},
pages = {941--949},
title = {{Regression Splines in the Cox Model with Application to Covariate Effects in Liver Disease}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10474965},
volume = {85},
year = {1990}
}

@misc{Parmar2018,
abstract = {Radiographic imaging continues to be one of the most effective and clinically useful tools within oncology. Sophistication of artificial intelligence has allowed for detailed quantification of radiographic characteristics of tissues using predefined engineered algorithms or deep learning methods. Precedents in radiology as well as a wealth of research studies hint at the clinical relevance of these characteristics. However, critical challenges are associated with the analysis of medical imaging data. Although some of these challenges are specific to the imaging field, many others like reproducibility and batch effects are generic and have already been addressed in other quantitative fields such as genomics. Here, we identify these pitfalls and provide recommendations for analysis strategies of medical imaging data, including data normalization, development of robust models, and rigorous statistical analyses. Adhering to these recommendations will not only improve analysis quality but also enhance precision medicine by allowing better integration of imaging data with other biomedical data sources.},
author = {Parmar, Chintan and Barry, Joseph D. and Hosny, Ahmed and Quackenbush, John and Aerts, Hugo J.W.L.},
booktitle = {Clinical Cancer Research},
doi = {10.1158/1078-0432.CCR-18-0385F},
issn = {15573265},
pmid = {29581134},
title = {{Data analysis strategies in medical imaging}},
year = {2018}
}
@misc{Yang2015,
author = {Yang, Christopher C. and Veltri, Pierangelo},
booktitle = {Artificial Intelligence in Medicine},
doi = {10.1016/j.artmed.2015.08.002},
issn = {18732860},
pmid = {26306669},
title = {{Intelligent healthcare informatics in big data era}},
year = {2015}
}
@article{Buhlmann2014,
abstract = {We review statistical methods for high-dimensional data analysis and pay particular attention to recent developments for assessing uncertainties in terms of controlling false positive statements (type I error) and p-values. The main focus is on regression models, but we also discuss graphical modeling and causal inference based on observational data.We illustrate the concepts and methods with various packages from the statistical software R using a high-throughput genomic data set about riboflavin production with Bacillus subtilis, which we make publicly available for the first time. {\textcopyright} 2014 by Annual Reviews.},
author = {B{\"{u}}hlmann, Peter and Kalisch, Markus and Meier, Lukas},
doi = {10.1146/annurev-statistics-022513-115545},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/6579b83470650967b0c903e6af005ec10685.pdf:pdf},
issn = {2326-8298},
journal = {Annual Review of Statistics and Its Application},
keywords = {Causal inference,Graphical modeling,Multiple testing,Penalized estimation,Regression},
month = {jan},
number = {1},
pages = {255--278},
title = {{High-Dimensional Statistics with a View Toward Applications in Biology}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-022513-115545},
volume = {1},
year = {2014}
}

@article{Fan2001,
abstract = {Variable selection is fundamental to high-dimensiona l statistical modeling, including nonparametri c regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametri c modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/penlike.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hard thresholding,lasso,nonnegative garrote,oracle estimator,penalized likelihood,scad,soft thresholding},
month = {dec},
number = {456},
pages = {1348--1360},
pmid = {21796725},
title = {{Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
volume = {96},
year = {2001}
}

@article{NengjunYi2013,
author = {Mallick, Himel and Yi, Nengjun},
doi = {10.4172/2155-6180.S1-005},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manuscript - 2014 - NIH Public Access.pdf:pdf},
issn = {21556180},
journal = {Journal of Biometrics {\&} Biostatistics},
keywords = {bayesian,bayesian hierarchical models,bayesian model selection,bayesian subset regression,consistency,high dimensional linear models,mcmc,nonlocal priors,penalized regression,posterior,regularization,shrinkage methods,variable selection},
number = {205},
pages = {1--27},
title = {{Bayesian Methods for High Dimensional Linear Models}},
url = {https://www.omicsonline.org/bayesian-methods-for-high-dimensional-linear-models-2155-6180.S1-005.php?aid=15156},
year = {2013}
}

@article{OHara2009,
abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo {\&} Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Je{\textregistered}reys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their di{\textregistered}erent properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the di{\textregistered}erent methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
annote = {In common practice, the same data will be use to both select the variables and estimate effect sizes, which cause overestimation of effect size. However, Bayesian method provides an robust estimateion by average the effect size of different models.},
author = {O'Hara, R. B. and Sillanp{\"{a}}{\"{a}}, M. J.},
doi = {10.1214/09-BA403},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hara, Sillanp{\"{a}}{\"{a}} - 2009 - A review of bayesian variable selection methods What, how and which.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {BUGS,MCMC,Variable selection},
number = {1},
pages = {85--118},
title = {{A review of bayesian variable selection methods: What, how and which}},
volume = {4},
year = {2009}
}

@incollection{Tipping2004,
abstract = {This article gives a basic introduction to the principles of Bayesian inference in a machine learning context, with an emphasis on the importance of marginalisation for dealing with uncertainty. We begin by illustrating concepts via a simple regression task before relating ideas to practical, contemporary, techniques with a description of ‘sparse Bayesian' models and the ‘relevance vector machine'},
author = {Tipping, Michael E.},
booktitle = {Summer School on Machine Learning},
doi = {10.1007/978-3-540-28650-9_3},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tipping - 2004 - Bayesian Inference An Introduction to Principles and Practice in Machine Learning.pdf:pdf},
pages = {41--62},
title = {{Bayesian Inference: An Introduction to Principles and Practice in Machine Learning}},
url = {http://www.miketipping.com/papers.htm http://link.springer.com/10.1007/978-3-540-28650-9{\_}3},
volume = {6},
year = {2004}
}

@article{Park2008,
abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
author = {Park, Trevor and Casella, George},
doi = {10.1198/016214508000000337},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Casella - 2008 - The Bayesian Lasso.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Empirical Bayes,Gibbs sampler,Hierarchical model,Inverse Gaussian,Linear regression,Penalized regression,Scale mixture of normals},
number = {482},
pages = {681--686},
title = {{The Bayesian Lasso}},
volume = {103},
year = {2008}
}


@article{Yi2008,
abstract = {The mapping of quantitative trait loci (QTL) is to identify molecular markers or genomic loci that influence the variation of complex traits. The problem is complicated by the facts that QTL data usually contain a large number of markers across the entire genome and most of them have little or no effect on the phenotype. In this article, we propose several Bayesian hierarchical models for mapping multiple QTL that simultaneously fit and estimate all possible genetic effects associated with all markers. The proposed models use prior distributions for the genetic effects that are scale mixtures of normal distributions with mean zero and variances distributed to give each effect a high probability of being near zero. We consider two types of priors for the variances, exponential and scaled inverse-$\chi$2 distributions, which result in a Bayesian version of the popular least absolute shrinkage and selection operator (LASSO) model and the well-known Student's t model, respectively. Unlike most applications where fixed values are preset for hyperparameters in the priors, we treat all hyperparameters as unknowns and estimate them along with other parameters. Markov chain Monte Carlo (MCMC) algorithms are developed to simulate the parameters from the posteriors. The methods are illustrated using well-known barley data. Copyright {\textcopyright} 2008 by the Genetics Society of America.},
author = {Yi, Nengjun and Xu, Shizhong},
doi = {10.1534/genetics.107.085589},
file = {:C$\backslash$:/Users/boyiguo1/Desktop/genetics1045.pdf:pdf},
issn = {00166731},
journal = {Genetics},
number = {2},
pages = {1045--1055},
pmid = {18505874},
title = {{Bayesian LASSO for quantitative trait loci mapping}},
volume = {179},
year = {2008}
}

@article{George1993,
author = {George, Edward I. and McCulloch, Robert E.},
doi = {10.1080/01621459.1993.10476353},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George, McCulloch - 1993 - Variable Selection via Gibbs Sampling.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {sep},
number = {423},
pages = {881--889},
title = {{Variable Selection via Gibbs Sampling}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476353},
volume = {88},
year = {1993}
}

@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George andMcCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa- rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for pos- terior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probabil- ity. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I. and McCulloch, Robert E.},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George, McCulloch - 1997 - Approaches for Bayesian variable selection.pdf:pdf},
journal = {Statistica Sinica},
keywords = {Gibbs sampling,Gray code,conjugate prior,hierar},
number = {2},
pages = {339--373},
title = {{Approaches for Bayesian variable selection.}},
volume = {7},
year = {1997}
}

@article{Brown1998,
abstract = {The multivariate regression model is considered with p regressors. A latent vector with p binary entries serves to identify one of two types of regression coefficients: those close to 0 and those not. Specializing our general distributional setting to the linear model with Gaussian errors and using natural conjugate prior distributions, we derive the marginal posterior distribution of the binary latent vector. Fast algorithms aid its direct computation, and in high dimensions these are supplemented by a Markov chain Monte Carlo approach to sampling from the known posterior distribution. Problems with hundreds of regressor variables become quite feasible. We give a simple method of assigning the hyperparameters of the prior distribution. The posterior predictive distribution is derived and the approach illustrated on compositional analysis of data involving three sugars with 160 near infra-red absorbances as regressors. {\textcopyright} 1998 Royal Statistical Society.},
author = {Brown, P. J. and Vannucci, M. and Fearn, T.},
doi = {10.1111/1467-9868.00144},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Vannucci, Fearn - 1998 - Multivariate Bayesian variable selection and prediction.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Bayesian selection,Conjugate distributions,Latent variables,Markov chain Monte Carlo method,Model averaging,Multivariate regression,Prediction},
number = {3},
pages = {627--641},
title = {{Multivariate Bayesian variable selection and prediction}},
volume = {60},
year = {1998}
}

@article{Chipman1996,
abstract = {In data sets with many predictors, algorithms for identifying a good subset of predictors are often used. Most such algorithms do not allow for any relationships between predictors. For example, stepwise regression might select a model containing an interaction AB but neither main effect A or B. This paper develops mathematical representations of this and other relations between predictors, which may then be incorporated in a model selection procedure. A Bayesian approach that goes beyond the standard independence prior for variable selection is adopted, and preference for certain models is interpreted as prior information. Priors relevant to arbitrary interactions and polynomials, dummy variables for categorical factors, competing predictors, and restrictions on the size of the models are developed. Since the relations developed are for priors, they may be incorporated in any Bayesian variable selection algorithm for any type of linear model. The application of the methods here is illustrated via the stochastic search variable selection algorithm of George and McCulloch (1993), which is modified to utilize the new priors. The performance of the approach is illustrated with two constructed examples and a computer performance dataset.},
author = {Chipman, Hugh},
doi = {10.2307/3315687},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chipman - 1996 - Bayesian variable selection with related predictors.pdf:pdf},
issn = {03195724},
journal = {Canadian Journal of Statistics},
keywords = {ams 1991 subject classification,and phrases,dummy variable,gibbs sampler,interaction,primary 62105,regression},
month = {mar},
number = {1},
pages = {17--36},
title = {{Bayesian variable selection with related predictors}},
url = {http://doi.wiley.com/10.2307/3315687},
volume = {24},
year = {1996}
}

@article{Ishwaran2005,
abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0505633v1},
author = {Ishwaran, Hemant and Rao, J. Sunil},
doi = {10.1214/009053604000001147},
eprint = {0505633v1},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishwaran, Rao - 2005 - Spike and slab variable selection Frequentist and Bayesian strategies.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
month = {apr},
number = {2},
pages = {730--773},
primaryClass = {arXiv:math},
title = {{Spike and slab variable selection: Frequentist and Bayesian strategies}},
url = {http://projecteuclid.org/euclid.aos/1117114335},
volume = {33},
year = {2005}
}

@article{Clyde2004,
abstract = {The evolution of Bayesian approaches for model uncertainty over the past decade has been remarkable. Catalyzed by advances in methods and technology for posterior computation, the scope of these methods has widened substantially. Major thrusts of these developments have included new methods for semiautomatic prior specification and posterior exploration. To illustrate key aspects of this evolution, the highlights of some of these developments are described.},
author = {Clyde, Merlise and George, Edward I.},
doi = {10.1214/088342304000000035},
file = {:C\:/Users/boyiguo1/Desktop/4144374.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayes factors,Classification arid regression trees,Linear and nonparametric regression,Model averaging,Objective prior distributions,Reversible jump Markov chain Monte Carlo,Variable selection},
number = {1},
pages = {81--94},
title = {{Model uncertainty}},
volume = {19},
year = {2004}
}


@article{Rockova2014a,
abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose {EMVS}, a deterministic alternative to stochastic search based on an {EM} algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posterior model probabilities, {EMVS} rapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the {EMVS} framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multi-modality associated with variable selection priors. The usefulness the {EMVS} approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
annote = {Rockova and George (2014) proposed an EM-based idea for the spike-and-slab Gaussian mixture prior, EMVS. The coefficent $\beta_j$ independently follows a mixture normal distribution $N(0, \sigma^2((1-\gamma_i)v_0 + \gamma_i v_1)$ for $0 \leq v_0 < v_1$. The parameter $\sigma^2$ follows a inverse gamma prior, $IG(v/2, v\lambda /2)$. Like in the spike-and slab family, the latent binary variables $\bm \gamma$ follows a bernoulli prior with a hyper prior $\theta$ follows a beta distribution, or simplifier uniform (0,1). In the E step, the latent variable are treated as the missing data, and calculate the prosterior mean of $\gamma$; in the M step, a ridge estimator was used to update the coefficients, and $\sigma^2, \theta$ are updated accordingly. As the ridge regression creates small but nonzero coeffcients, a further step is needed to select the varibale: EMVS further suggests to threshold the inclusion probabilty for variable selection or generate regularization plot with different values of the spike scale parameter. EMVS is also compatible with structured prior when dealling with grouped variables. The group structre are imposed on the prior of the latent binary variable $\gamma$ via either logistic regression or markov random field.},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1080/01621459.2013.869223},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}}, George - 2014 - EMVS The EM approach to Bayesian variable selection(2).pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Dynamic posterior exploration,High dimensionality,Regularization plots,SSVS,Sparsity},
number = {506},
pages = {828--846},
title = {{EMVS: The EM approach to Bayesian variable selection}},
volume = {109},
year = {2014}
}

@article{Rockova2018,
abstract = {Grounded theory analysis was applied to qualitative interviews with 25 communication professionals concerning cultural influences on crisis. This approach yielded several findings. First, public relations practitioners had difficulties in defining multiculturalism, often equating cultural diversity with communicating with Latinos. Second, interviewees saw cultural differences as just one aspect of diversity, emphasizing that age, religion, and education differences also affect corporate discourse. Third, although professionals considered culture a key element of crisis management, they did not feel prepared to handle the challenges of a multicultural crisis, nor did they report that they used culturally adjusted crisis strategies often. By integrating cultural competence and crisis management frameworks, this study provides the foundation for an in-depth understanding of crises, where scholars and professionals can pair crisis strategies with audiences' cultural expectations. Training initiatives focused on increasing levels of cultural competence can make organizations and communication professionals ready to the challenges of a global market. [ABSTRACT FROM PUBLISHER]},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1080/01621459.2016.1260469},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}}, George - 2018 - The Spike-and-Slab LASSO.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,Spike-and-Slab,Variable selection},
number = {521},
pages = {431--444},
publisher = {Taylor & Francis},
title = {{The Spike-and-Slab LASSO}},
url = {https://doi.org/10.1080/01621459.2016.1260469},
volume = {113},
year = {2018}
}

@article{Rockova2018b,
author = {Ro{\v{c}}kov{\'{a}}, Veronika},
doi = {10.1214/17-AOS1554},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}} - 2018 - Bayesian estimation of sparse signals with a continuous spike-and-slab prior.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {feb},
number = {1},
pages = {401--437},
title = {{Bayesian estimation of sparse signals with a continuous spike-and-slab prior}},
url = {https://projecteuclid.org/euclid.aos/1519268435},
volume = {46},
year = {2018}
}


@article{Tang2017,
abstract = {{\textcopyright} 2017 The Author. Motivation: Large-scale molecular profiling data have offered extraordinary opportunities to improve survival prediction of cancers and other diseases and to detect disease associated genes. However, there are considerable challenges in analyzing large-scale molecular data. Results: We propose new Bayesian hierarchical Cox proportional hazards models, called the spikeand- slab lasso Cox, for predicting survival outcomes and detecting associated genes. We also develop an efficient algorithm to fit the proposed models by incorporating Expectation-Maximization steps into the extremely fast cyclic coordinate descent algorithm. The performance of the proposed method is assessed via extensive simulations and compared with the lasso Cox regression. We demonstrate the proposed procedure on two cancer datasets with censored survival outcomes and thousands of molecular features. Our analyses suggest that the proposed procedure can generate powerful prognostic models for predicting cancer survival and can detect associated genes. Availability and implementation: The methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/).},
author = {Tang, Zaixiang and Shen, Yueping and Zhang, Xinyan and Yi, Nengjun},
doi = {10.1093/bioinformatics/btx300},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2017 - The spike-and-slab lasso Cox model for survival prediction and associated genes detection.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {18},
pages = {2799--2807},
title = {{The spike-and-slab lasso Cox model for survival prediction and associated genes detection}},
volume = {33},
year = {2017}
}
@article{Tang2017a,
abstract = {{\textcopyright} 2017 by the Genetics Society of America. Large-scale “omics” data have been increasingly used as an important resource for prognostic prediction of diseases and detection of associated genes. However, there are considerable challenges in analyzing high-dimensional molecular data, including the large number of potential molecular predictors, limited number of samples, and small effect of each predictor. We propose new Bayesian hierarchical generalized linear models, called spike-and-slab lasso GLMs, for prognostic prediction and detection of associated genes using large-scale molecular data. The proposed model employs a spike-and-slab mixture double-exponential prior for coefficients that can induce weak shrinkage on large coefficients, and strong shrinkage on irrelevant coefficients. We have developed a fast and stable algorithm to fit large-scale hierarchal GLMs by incorporating expectation-maximization (EM) steps into the fast cyclic coordinate descent algorithm. The proposed approach integrates nice features of two popular methods, i.e., penalized lasso and Bayesian spike-and-slab variable selection. The performance of the proposed method is assessed via extensive simulation studies. The results show that the proposed approach can provide not only more accurate estimates of the parameters, but also better prediction. We demonstrate the proposed procedure on two cancer data sets: a well-known breast cancer data set consisting of 295 tumors, and expression data of 4919 genes; and the ovarian cancer data set from TCGA with 362 tumors, and expression data of 5336 genes. Our analyses show that the proposed procedure can generate powerful models for predicting outcomes and detecting associated genes. The methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/).},
author = {Tang, Zaixiang and Shen, Yueping and Zhang, Xinyan and Yi, Nengjun},
doi = {10.1534/genetics.116.192195},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2017 - The spike-and-slab lasso generalized linear models for prediction and associated genes detection.pdf:pdf},
issn = {19432631},
journal = {Genetics},
keywords = {Cancer,Double-exponential distribution,Generalized linear model,Outcome prediction,Spike-and-slab lasso},
number = {1},
pages = {77--88},
title = {{The spike-and-slab lasso generalized linear models for prediction and associated genes detection}},
volume = {205},
year = {2017}
}
@article{Tang2019,
abstract = {Group structures among genes encoded in functional relationships or biological pathways are valuable and unique features in large-scale molecular data for survival analysis. However, most of previous approaches for molecular data analysis ignore such group structures. It is desirable to develop powerful analytic methods for incorporating valuable pathway information for predicting disease survival outcomes and detecting associated genes. We here propose a Bayesian hierarchical Cox survival model, called the group spike-and-slab lasso Cox (gsslasso Cox), for predicting disease survival outcomes and detecting associated genes by incorporating group structures of biological pathways. Our hierarchical model employs a novel prior on the coefficients of genes, i.e., the group spike-and-slab double-exponential distribution, to integrate group structures and to adaptively shrink the effects of genes. We have developed a fast and stable deterministic algorithm to fit the proposed models. We performed extensive simulation studies to assess the model fitting properties and the prognostic performance of the proposed method, and also applied our method to analyze three cancer data sets. Both the theoretical and empirical studies show that the proposed method can induce weaker shrinkage on predictors in an active pathway, thereby incorporating the biological similarity of genes within a same pathway into the hierarchical modeling. Compared with several existing methods, the proposed method can more accurately estimate gene effects and can better predict survival outcomes. For the three cancer data sets, the results show that the proposed method generates more powerful models for survival prediction and detecting associated genes. The method has been implemented in a freely available R package BhGLM at 
                    https://github.com/nyiuab/BhGLM
                    
                  .},
author = {Tang, Zaixiang and Lei, Shufeng and Zhang, Xinyan and Yi, Zixuan and Guo, Boyi and Chen, Jake Y. and Shen, Yueping and Yi, Nengjun},
doi = {10.1186/s12859-019-2656-1},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2019 - Gsslasso Cox A Bayesian hierarchical model for predicting survival and detecting associated genes by incorporating.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Cox survival models,Grouped predictors,Hierarchical modeling,Lasso,Pathway,Spike-and-slab prior},
number = {1},
pages = {1--15},
publisher = {BMC Bioinformatics},
title = {{Gsslasso Cox: A Bayesian hierarchical model for predicting survival and detecting associated genes by incorporating pathway information}},
volume = {20},
year = {2019}
}
@article{Tang2018,
abstract = {MotivationLarge-scale molecular data have been increasingly used as an important resource for prognostic prediction of diseases and detection of associated genes. However, standard approaches for omics data analysis ignore the group structure among genes encoded in functional relationships or pathway information.ResultsWe propose new Bayesian hierarchical generalized linear models, called group spike-and-slab lasso GLMs, for predicting disease outcomes and detecting associated genes by incorporating large-scale molecular data and group structures. The proposed model employs a mixture double-exponential prior for coefficients that induces self-adaptive shrinkage amount on different coefficients. The group information is incorporated into the model by setting group-specific parameters. We have developed a fast and stable deterministic algorithm to fit the proposed hierarchal GLMs, which can perform variable selection within groups. We assess the performance of the proposed method on several simulated scenarios, by varying the overlap among groups, group size, number of non-null groups, and the correlation within group. Compared with existing methods, the proposed method provides not only more accurate estimates of the parameters but also better prediction. We further demonstrate the application of the proposed procedure on three cancer data sets by utilizing pathway structures of genes. Our results show that the proposed method generates powerful models for predicting disease outcomes and detecting associated genes.AvailabilityThe methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/).Contactnyi@uab.eduSupplementary informationSupplementary data are available at Bioinformatics online.},
author = {Tang, Zaixiang and Shen, Yueping and Li, Yan and Zhang, Xinyan and Wen, Jia and Qian, Chen'Ao and Zhuang, Wenzhuo and Shi, Xinghua and Yi, Nengjun},
doi = {10.1093/bioinformatics/btx684},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2018 - Group spike-And-slab lasso generalized linear models for disease prediction and associated genes detection by incor.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {6},
pages = {901--910},
title = {{Group spike-And-slab lasso generalized linear models for disease prediction and associated genes detection by incorporating pathway information}},
volume = {34},
year = {2018}
}

@article{Bai2020,
abstract = {{High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations}. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real data sets.},
archivePrefix = {arXiv},
arxivId = {2010.06451},
author = {Bai, Ray and Rockova, Veronika and George, Edward I.},
eprint = {2010.06451},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Rockova, George - 2020 - Spike-and-Slab Meets LASSO A Review of the Spike-and-Slab LASSO.pdf:pdf},
keywords = {and phrases,high-dimensional data,slab lasso,sparsity,spike-and-,spike-and-slab,variable selection},
month = {oct},
pages = {1--31},
title = {{Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO}},
url = {http://arxiv.org/abs/2010.06451},
year = {2020}
}

@article{Bai2020Spline,
abstract = {We introduce a general framework for estimation and variable selection in high-dimensional Bayesian generalized additive models where the response belongs to the overdispersed exponential family. Our framework subsumes popular models such as binomial regression, Poisson regression, Gaussian regression, and negative binomial regression, and encompasses both canonical and non-canonical link functions. Our method can be implemented with a highly efficient EM algorithm, allowing us to rapidly attain estimates of the significant functions while thresholding out insignificant ones. Under mild regularity conditions, we establish posterior contraction rates and model selection consistency when the number of covariates grows at nearly exponential rate with sample size. We illustrate our method on both synthetic and real data sets.},
annote = {Bai proposed an novel high dimenional Bayesian GAM using the spike-and-slab group lasso prior. An fast fitting EM-based algorithm is proposed. Asympotoic threory, such as posterior contraction rates and model selction consistency are proved.

Even though, the algorithm is flexible to cover many non-Gaussina family in a high dimensional setting, the proposed method didn't address uncertianty quantification, i.e. calculating credile intervals etc.},
archivePrefix = {arXiv},
arxivId = {2007.07021},
author = {Bai, Ray},
eprint = {2007.07021},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai - 2020 - A Unified Computational and Theoretical Framework for High-Dimensional Bayesian Additive Models.pdf:pdf},
keywords = {and phrases,generalized additive model,non-gaussian data,rate of con-,spike-and-slab group lasso,variable selection,vergence},
month = {jul},
pages = {1--36},
title = {{A Unified Computational and Theoretical Framework for High-Dimensional Bayesian Additive Models}},
url = {http://arxiv.org/abs/2007.07021},
year = {2020}
}

@article{Ravikumar2009,
abstract = {We present a new class of methods for high dimensional non-parametric regression and classification called sparse additive models. Our methods combine ideas from sparse linear modelling and additive non-parametric regression. We derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. Sparse additive models are essentially a functional version of the grouped lasso of Yuan and Lin. They are also closely related to the COSSO model of Lin and Zhang but decouple smoothing and sparsity, enabling the use of arbitrary non-parametric smoothers. We give an analysis of the theoretical properties of sparse additive models and present empirical results on synthetic and real data, showing that they can be effective in fitting sparse non-parametric models in high dimensional data. {\textcopyright} 2009 Royal Statistical Society.},
annote = {Ravikumar, Lafferty, Lue and Wasserman, extended the orginal backfitting algorithm to accommadate the high-dimensional setting. The sparsity and smoothness was handled saparately with a softthreshold. The authors also deomonstrated its similarity to Group Lasso},
archivePrefix = {arXiv},
arxivId = {0711.4555},
author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman, Larry},
doi = {10.1111/j.1467-9868.2009.00718.x},
eprint = {0711.4555},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravikumar et al. - 2009 - Sparse additive models.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Additive models,Lasso,Non-parametric regression,Sparsity},
month = {nov},
number = {5},
pages = {1009--1030},
title = {{Sparse additive models}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00718.x},
volume = {71},
year = {2009}
}

@article{Meier2009,
abstract = {We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of our sparsity-smoothness penalized approach yields large additional performance gains. {\textcopyright} Institute of Mathematical Statistics, 2009.},
annote = {Meier, Van De Geer and BNuhlmann (2009) proposed an a spartsity-smoothness penalty,
$$
J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda \int(f^"_j(x))^2dx},},
archivePrefix = {arXiv},
arxivId = {0806.4115},
author = {Meier, Lukas and {Van De Geer}, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1214/09-AOS692},
eprint = {0806.4115},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meier, Van De Geer, B{\"{u}}hlmann - 2009 - High-dimensional additive modeling.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Group lasso,Model selection,Nonparametric regression,Oracle inequality,Penalized likelihood,Sparsity},
number = {6 B},
pages = {3779--3821},
title = {{High-dimensional additive modeling}},
volume = {37},
year = {2009}
}

@article{Scheipl2012,
abstract = {Structured additive regression (STAR) provides a general framework for complex Gaussian and non-Gaussian regression models, with predictors comprising arbitrary combinations of nonlinear functions and surfaces, spatial effects, varying coefficients, random effects, and further regression terms. The large flexibility of STAR makes function selection a challenging and important task, aiming at (1) selecting the relevant covariates, (2) choosing an appropriate and parsimonious representation of the impact of covariates on the predictor, and (3) determining the required interactions. We propose a spike-and-slab prior structure for function selection that allows to include or exclude single coefficients as well as blocks of coefficients representing specific model terms. A novel multiplicative parameter expansion is required to obtain good mixing and convergence properties in a Markov chain Monte Carlo simulation approach and is shown to induce desirable shrinkage properties. In simulation studies and with (real) benchmark classification data, we investigate sensitivity to hyperparameter settings and compare performance to competitors. The flexibility and applicability of our approach are demonstrated in an additive piecewise exponential model with time-varying effects for right-censored survival times of intensive care patients with sepsis. Geoadditive and additive mixed logit model applications are discussed in an extensive online supplement. {\textcopyright} 2012 American Statistical Association.},
annote = {semiparametric regression model can be solved when treating as mixed models. Scheipl, Fahrmeir, Kneib (2012) proposed an new spike-and-slab structure prior, normal-mixture -of-inverse gamma (NMIG) for semiparmtric regression using MCMC. The NMIG prior is mixtrue of t-distribution with an additional coerfficients mixing vector, where the vector follows a mixtureof two Gaussian distribution.},
archivePrefix = {arXiv},
arxivId = {1105.5250},
author = {Scheipl, Fabian and Fahrmeir, Ludwig and Kneib, Thomas},
doi = {10.1080/01621459.2012.737742},
eprint = {1105.5250},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheipl, Fahrmeir, Kneib - 2012 - Spike-and-slab priors for function selection in structured additive regression models.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Generalized additive mixed models,Parameter expansion,Penalized splines,Spatial regression,Stochastic search variable selection},
number = {500},
pages = {1518--1532},
title = {{Spike-and-slab priors for function selection in structured additive regression models}},
volume = {107},
year = {2012}
}

@book{Deboor1978,
  title={A practical guide to splines},
  author={De Boor, Carl},
  volume={27},
  year={1978},
  publisher={springer-verlag New York}
}

@article{Mitchell1988,
abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a “spike and slab” distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation $\sigma$, where ln($\sigma$) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter $\gamma$, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing $\gamma$, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against $\gamma$ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose y. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of “leave one out” approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods. {\textcopyright} 1976 Taylor & Francis Group, LLC.},
author = {Mitchell, T. J. and Beauchamp, J. J.},
doi = {10.1080/01621459.1988.10478694},
file = {:C\:/Users/boyiguo1/Desktop/Bayesian Variable Selection in Linear Regression.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Cross-validation,Linear models,Subset selection},
number = {404},
pages = {1023--1032},
title = {{Bayesian variable selection in linear regression}},
volume = {83},
year = {1988}
}

@article{keller2018genetic,
  title={Genetic drivers of pancreatic islet function},
  author={Keller, Mark P and Gatti, Daniel M and Schueler, Kathryn L and Rabaglia, Mary E and Stapleton, Donnie S and Simecek, Petr and Vincent, Matthew and Allen, Sadie and Broman, Aimee Teo and Bacher, Rhonda and others},
  journal={Genetics},
  volume={209},
  number={1},
  pages={335--356},
  year={2018},
  publisher={Oxford University Press}
}



