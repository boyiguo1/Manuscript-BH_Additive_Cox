\section{Cox Proportional Hazard Additive Model}

For each individual, we collect the the covariate variables $\bs X_i = (X_{i1}, \dots, X_{ip}) \in \mathbb{R}^{p}$ and the survival outcome $(T_i, C_i) \in {\mathbb{R}^+ \times \{0,1\}}$ where $T$ is the observed survival time, and $C$ is a binary variable, marking if the status. When $C=0$, the observation is censored at the survival time $T$ and when $C=1$, we observe the event of interest at time $T$. We assume non-informative right censoring. We also assume there is no competing risk, or multiple occurrence of the event. For the $i$th individual, a tuple of three exists $\{t_i, d_i, \bs x_i\}$, where $\bs x_i$ is a vector of covariates. In addition, the survival time $t_i$ and the censoring indicator $d_i$ are recorded, where $d_i$ takes the value 1 when censoring happens at the survival time $y_i$, and takes the value 0 when the event of interest happens. The Cox proportional hazard model, with additive functions $B_j(x_j) = \beta_jX_j$, is formulated as $$
h(t|\bs x) = h_0(t)exp(a + \sum\limits^p_{j=1}B_j(x_j)) = h_0(t) \exp(a + \sum\limits^p_{j=1}\beta^T_jX_j).
$$ For the purpose of identifiability, we impose an constraint on each additive function $E[B_j(x_j)] = 0$. $h(t)$ is the hazard function and $h_0(t)$ is the baseline hazard function. The hazard function $h(t)$ describes the instantaneous rate of event occurrence among people who are still at risk at the moment. It has a one-to-one relationship with the survival function, which describes the distribution of event time. The linear predictor $\bs x \bs \beta$ is also called the **prognostic index**. To be noted, different from GLMs, there is no intercept term necessary in the design matrix $\bs x$, as the baseline hazard function $h_0(t)$ estimates an reference level of survival risk. The baseline hazard function indeed changes when additional measure were taken fro the design matrix, such as centering and scaling, while the coefficients $\bs x$ remain unchanged. We defer to \cite{Klein2003, Ibrahim2001} for in-depth survey of the topic.

Unlike the GLM, the estimation for the coefficients $\bs \beta$ doesn't directly maximize log likelihood due to the functional nature of survival analysis. Instead, we consider to maximize the partial log-likelihood function \cite{cox1972}, mathematically, $$
pl(\bs \beta) = \sum\limits^n_{i=1}d_i\log\frac{\exp{\bs x_i \bs \beta}}{\sum_{j\in R(t_i)} \exp(\bs x_j \bs \beta)},
$$ where $R(t_i)$ denotes the risk set at time $t_i$, i.e. the set of all patients who still survived prior to time $t_i$. When tied failure or censoring time exists, a modified partial log-likelihood function can be used \cite{Efron1977}. Conditional on $\beta$, the baseline hazard function $h_0(t_i)$ can be estimated using Breslow estimator \cite{breslow1974}, $$
\hat h_0(t_i|\beta) = d_i/\sum\limits_{i^\prime \in R(t_i)} exp(X_{i^\prime}\beta).
$$

To encourage proper smoothing of the functions, we adopt the idea of smoothing penalties from smoothing spline models \cite{Wood2017}. Smoothing penalty is conventionally formulated as integrated squared second derivative of smooth functions. It's advantages[TODO: two sentences to describe smooth penalty] While it is hard to directly integrate smoothing penalty with sparsity penalty, \cite{Marra2011} proposed a reparameterization to implement smoothing penalty in the design matrix. Given the smoothing penalty matrix $\bs S_j$ is symmetric and positive semi-definite for univariate smoothing functions, we apply eigendecomposition on the penalty matrix $\bs S = \bs U \bs D \bs U^T$ , where eigenvectors and eigenvalues are arranged in the matrices $\bs U$ and $\bs D$ respectively. The zero eigenvalue and its corresponding eigenvector span the linear space of the smoothing function, which allows us to separate the linear space from the smoothing function. By multiplying the design matrix $X$ and eigenvector matrix $U$ and properly scaling with eigenvalues, we can have a new design matrix such that the smoothing function of variable $x_j$ can be written as $$
B_j(x_j) = B_j^0(x_j) + B_j^\tp (x_j) = \beta_j X^0_j + \bs {\beta_j^\tp}^T \bs X_j^\tp ,
$$ where $\bs X_j \equiv \begin{bmatrix} X_j^0 : \bs X_j^{\tp} \end{bmatrix}$ as the basis function matrix for the $j$th variable; the coefficients $\bs \beta_j \equiv \begin{bmatrix} \beta_j : \bs \beta^\tp_j \end{bmatrix}$ is an augmentation of the coefficient scalar $\beta_j$ of linear space and the coefficient vector $\bs \beta^\tp_j$ of non-linear space.

\subsection{Two-part Spike-and-slab LASSO Prior for Smoothing Functions}

To model each additive function, we propose the two-part spike-and-slab LASSO prior for smooth functions under the additive Cox proportional hazard framework. The proposed prior can be considered as an extension of the previous spike-and-slab Lasso prior for group predictors \cite{tang2019}, and has been applied to the GAM settings \cite{guo2022}. The extension focus on

-   separate selection of the linear and nonlinear components of each additive function

-   effective smoothing of the nonlinear effect when exists.

-   Locally adaptive shrinkage on the coefficients. Specifically, the spike-and-slab LASSO prior suppress the weak signal to zero. It provides a natural process for the functional selection in comparison to some other Bayesian regularized prior, for example, spike-and-slab pior, that relies on thresholding.

-   It motivates a scalable model fitting algorithm for high-dimensional data analysis.

To recall, the spike-and-slab LASSO prior \cite{rockova2018a, rockova2018b} is a mixture double exponential prior with a spike density $DE(0, s_0)$ for small effects and a slab density $DE(0, s_1)$ for large effect. Like any other spike and slab priors, the spike is to contain the minimum to zero effects, while the slab is to allow large effects. The scale parameters $s_0$ and $s_1$ are also considered as tuning parameters, which can be optimized via cross-validation. A discussion of how to choose the scale parameters comes later. $s_0$ and $s_1$ are scale parameters, assuming given and $0 < s_0 < s_1$.  A latent indicator variable $\gamma_j \in (0,1)$ controls if the the predictor should be include in the model or not. $$
\beta_j|\gamma_j \sim (1-\gamma_j) DE(0, s_0) + \gamma_j DE(0, s_1), 0 < s_0 < s_1.
$$

To accommodate the group structure of the predictors, the group spike-and-slab LASSO prior \cite{tang2018, tang2019} makes the extension by imposing a group specific Bernoulli dsitribution for the indicator variables,

$$
\gamma_j|\theta_g \sim Bin(1, \theta_g).
$$

The probability parameter $\theta_g$ of group $g$ allows information borrowing across different predictors in the same group, and decides if the group is contributing to outcome. The group-specific hyperprior is built on the premise that if one predictor in the group is included in the model, the rest of the predictors are also more likely to be included in the model. When the group structure doesn't exists, we can treat each predictor to be in their own group, and hence, the spike-and-slab LASSO prior can be treated as a special case of the group spike-and-slab LASSO prior.

In the proposed two-part spike-and-slab LASSO prior for smooth function, we leverage the previous group spike-and-slab LASSO prior and make modification, such as group latent indicator and effect hierarchy principle. Specifically, we impose conditionally independent group SSLs on the linear and nonlinear components of a smoothing function. This is to leverage the fact that the bases of a smooth function have an natural group structure. Given the model matrix of the predictor $X_j$ after the reparameterization step introduce in Section 2.1, we have the linear and nonlinear component of the smoothing function $X_j^0$ and ${\bs X_j}^\tp$. The corresponding coefficients are $\bs \beta_j = [\beta_j:\bs \beta_j^\tp]$. We impose the group SSL priors on the linear and nonlinear components respectively,
\begin{align}
  \beta_{j} | \gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \nonumber \\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma_{j}^\tp) s_0 + \gamma_{j}^\tp s_1), k=1,\dots, K_j.
\end{align}
To note, we assume that the linear components are one-dimensional and hence using the special case, SSL prior, for simplicity. We make slight modification on the previous group SSL: we force all coefficients of the nonlinear components, i.e. within the same group, to share the same latent binary indicator. This encourages the inclusion of nonlinear components all together and hence, the bi-level selection. Meanwhile, we see that the all the nonlinear components should have the same magnitude of shrinkage after reparameterization, particularly the scaling, which can be considered as the smoothing penalty in the smoohting spline setting.

While the two latent indicators $\gamma_j$ and $\gamma_j^\tp$ controls the inclusion of the linear and nonlinear components of a smooth function, we still need to set up  some ordering of the inclusion. For example, it is normally assume that the lower-order effects are more likely to be active than the high-reorder effects (referred to as \textit{effect hierachy} \cite{chipman2006}). In order to implement the effect hierarchy principle in the bi-level selection, we further impose a dependent structure on the latent indicators,
\begin{align}
&\gamma_{j} | \theta_j \sim Bin(1, \theta_j) & & 
&\gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j).
\end{align}
This is the inclusion of the nonlinear component depends on the inclusion of the linear component. To note, one can easily relax the effect hierarchy by having the two latent indicators be independent condition on the inclusion probability parameter $\theta_j$. The two versions of the indicator prior could introduce trade-off in the variable selection (previously seen in \cite{guo2022}) and will be discussed more in the Section 5. To leverage the fact that the probability of selecting the bases of a smooth function should be similar, we have the linear and nonlinear components (if selected) share the inclusion probability $\theta_j$. It is also possible to have the linear and nonlinear coefficents shares the same indicators, i.e. $\beta_{jk}^\tp$ also depends on $\gamma_j$. However, this approach would disable the bi-level selection ability and force the nonlinear components uses sparsity shrinkage instead of smoothing shrinkage. Hence, it would reduce to a more strict version of group spike-and-slab LASSO where all coefficients in a group employs the same shrinkage instead of locally adaptive shrinkage, and hence it is not recommend here.

The rest of the proposed prior follows the spike-and-slab priors. The inclusion probability parameter $\theta_j$ independently and identically follows a $Beta(a, b)$ distribution. One can consider a special case of the $Beta$ distribution, $Uniform(0,1)$, for simplicity. The prior distribution of the inclusion probability parameter motivates the self-adapative shrinkage for signal sparsity and functional smoothness based on the data. In addition, being a conjugate prior of the binomial distribution, the $Beta$ prior can provide a closed-form solution in the following model fitting algorithm and mitigate some computational burdens. To note, when the variable have large effects in any of the bases, the parameter $\theta_j$ will be estimated large, which in turn encourages the model to include the rest of bases. 

<!-- The smoothness of each spline function will be further estimated via the individual $\gamma_jk$. Hereafter, we refer the hierarchical spline Cox Model with the group spike-and-slab mixture normal prior as the ss-Cox Spline Model. -->

\subsection{EM-Coordinate Descent Algorithm for Scalable Model Fitting}

Parsimonious computation is always encouraged in high-dimension data analysis. Bayesian methodology loses its advantages over Frequentest penalized model because of the computation cost. Previous Bayesian spline models heavily relies on the MCMC algorithm to establish posterior distribution of parameters. One of the exception is Bai (in progress), who took advantages of the fast computing EMVS algorithm and extended to the non-Gaussian GAM settings. He focused on the maximum a postiori (MAP) estimator of the parameters. While Bai's algorithm is fast, it lacks the ability to provide uncertainty measure of the estimates.

[A paragraph to introduce spike-and-slab.] Natural disadvantage, for example, prohibited coputational of model fitting algorithm, of Bayesian models doesn't discourage the development of bayesian models in Cox model.

We develop a fast deterministic algorithm to fit the ss-Cox model. The algorithm is an extension of the previously proposed EM-IRLS. It incorporates the IRLS algorithm into the EM steps to iteratively find the MAP estimates. Thanks to the versatility of IRLS algorithm, the proposed algorithm can dealt with many family of outcomes. Also, the proposed can work not only for the prior previously mentioned, but also some modified priors, for example mixed double exponential.

Recall the mixture normal prior set-up described , we have the prior densities, $$
\begin{aligned}
p(\bs \beta | \bs\gamma) &\propto \prod\limits_{j=1}^{p}\prod\limits_{k=1}^{K_j}((1-\gamma_{jk}) s_0 + \gamma_{jk} s_1)^{-1/2}\exp(-1/2(\beta_{jk}^2/((1-\gamma_{jk}) s_0 + \gamma_{jk} s_1)))\\
p(\bs\gamma | \bs \theta) & = \prod\limits_{j=1}^{p}\prod\limits_{k=1}^{K_j} \theta_j^{\gamma_{jk}} (1-\theta_j)^{1-\gamma_{jk}}.
\end{aligned}
$$

We define the parameters of interest as $\Theta = {\bs \beta, \bs \theta, \phi}$. The log posterior density of $\Theta$ requires a slight modification from the The log-posterior for ss-Cox model (up to additive constants). The log likelihood function needs to be replaced by the log partial likelihood function. The log density is still expressed with a two-part equation, which can be maximized separately,

$$ \log p(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) = Q_1(\beta, \phi) + Q_2 (\gamma,\theta),$$ Where $$ Q_1(\bs \beta, \phi) = \log pl(\textbf{y}|\bs \beta, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\sum\limits_{k=1}^{K_j} \log p(\beta_{jk}|\gamma_{jk})$$ and $$
Q_2(\delta,\theta) = \sum\limits_{j=1}^{p}\sum\limits_{k=1}^{K_j} \left[ \gamma_{jk}\log \theta_j + (1-\gamma_{jk}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j) .$$ The partial log-likelihood $pl(\beta)$ can also be substitute by the Breslow or the Effron approximation in the presence of ties., and the baseline hazard function $\lambda_0$ is replaced by the Breslow estimator.

Similar to the ssGAMs, we need to calculate the expecataion of the $Q_1$ and $Q_2$ with respect to the missing data $\gamma_{jk}|\Theta^{t-1}$, which yields the same estimation, \begin{align*}
E(Q_1) &= \log p(\textbf{y}|\bs \beta, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\sum\limits_{k=1}^{K_j}E(S^{-1}_{jk})\beta_{jk}^2\\
E(Q_2) &= \sum\limits_{j=1}^{p}\sum\limits_{k=1}^{K_j} \left[ E(\gamma_{jk})\log \theta_j + (1-E(\gamma_{jk})) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j)
\end{align*} where $$
E(\gamma_{jk}) = p_{jk} = \frac{Pr(\gamma_{jk} = 1|\theta_j)f(\beta_{jk}|\gamma_{jk}=1, s_1) }{Pr(\gamma_{jk} = 1|\theta_j)f(\beta_{jk}|\gamma_{jk}=1, s_1) + Pr(\gamma_{jk} = 0|\theta_j)f(\beta_{jk}|\gamma_{jk}=0, s_0)}
$$ $$
E(S^{-1}_{jk}) = \frac{1-p_{jk}}{2s_0} + \frac{p_{jk}}{2s_1}.
$$ We can see that the estimates of $\gamma_jk, S^{-1}_{jk}$ are larger for larger coefficients $\beta_{jk}$, leading to different shrinkage for different coefficients. Moreover, to note that, we have different shrinkage $S^{-1}_{jk}$for the coefficients $\beta_{jk}$ of the variable $x_j$, and hence, we can penalize the null space of the spline differently and allow local adaption.

In the M-step, $\bs \beta, \phi$ and $\bs \theta$ can be maximized respectively via $E(Q_1)$ and $E(Q_2)$ as $Q_1$ and $Q_2$ are functions contain either $\bs \beta$ or $\bs \theta$. Therefore, the coefficients $\bs \beta$ are updated by maximizing $E(Q_1)$, where $E(Q_1)$ can be treated as $L_2$ penalized likelihood function. Such maximization can be easily achieved with IRLS. The disperse parameter $\phi$ is updated if applicable. Meanwhile, the probability parameters $\bs \theta$ are updated by maximizing $E(Q_2)$. As the beta distribution is a conjugate prior for Bernoulli distribution, $\bs \theta$ can be easily updated with a closed form equation: $$
\theta_j = \frac{\sum\limits_{k = 1}^{K_j}p_{jk} + a - 1 }{K_j + a + b -2}.
$$ The last step is

Totally, the framework of the proposed EM IRLS algorithm was summarized as follows:

1)  Choose a starting value $\bs \beta^0$ and $\bs \theta^0$ for $\bs \beta$ and $\bs \theta$. For example, we can initialize $\bs \beta^0 = \bs 0$ and $\bs \theta^0 = \bs 0.5$

2)  Iterate over the E-step and M-step until convergence

E-step: calculate $E(\gamma_{jk})$ and $E(S^{-1}_{jk})$ with estimates of $\Theta$ from previous iteration

M-step:

a)  Update $\bs \beta$ using the IRLS algorithm

b)  Update $\bs \theta$ using the closed form equation

We assess convergence by the criterion: $|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon$, where $d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})$ is the estimate of deviance at the $t^{\text{th}}$ iteration, and $\epsilon$ is a small value (say $10^{-5}$).
