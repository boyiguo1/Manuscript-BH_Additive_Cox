# Cox Proportional Hazard Additive Model

For each individual in the datasets, we collect the the covariate variables $\bs X_i = (X_{i1}, \dots, X_{ip}) \in \mathbb{R}^{p}$ and the survival outcome $(T_i, C_i) \in {\mathbb{R}^+ \times \{0,1\}}$ where $T$ is the observed survival time, and $C$ is a binary variable, marking if the status. When $C=0$, the observation is censored at the survial time $T$ and when $C=1$, we observe the event of interest at time $T$. We assume non-informative right censoring. We also assume there is no competing risk, or multiple occurrence of the event.

The most popular tools to analyze such data include Cox proportional hazard model, refered as Cox model hereafter. Cox model is a semi-parametric model that assumes proportional hazard. Each participant shares a common baseline hazard function of time $t$, $h_0(t)$, and the individual hazard given their covariates $\bs x_i$ change on the multiplicative scale, mathematically,
$$
h(t;\bs x_i) = h_0(t)\exp(\bs \beta^T \bs x_i).
$$
To note, intercept term are normally replaced with the baseline hazard function $h_0(t)$.


- Introduce the reparmeterization

To encourage proper smoothing of the functions, we adopt the idea of smoothing penalties from smoothing spline models [@Wood2017].
<!-- The basic idea is to set up a smoothing penalty, described in Equation (\ref{eq:smoothpen}), in the prior density function. However, the direct integration of smoothing penalty with sparsity penalty is not obvious. -->
While it is hard to directly integrate smoothing penalty with sparsity penalty, @Marra2011 proposed a reparameterization to implement smoothing penalty in the design matrix. Given the smoothing penalty matrix $\bs S_j$ is symmetric and positive semi-definite for univariate smoothing functions, we apply eigendecomposition on the penalty matrix $\bs S = \bs U \bs D \bs U^T$ , where eigenvectors and eigenvalues are arranged in the matrices $\bs U$ and $\bs D$ respectively. The zero eigenvalue and its corresponding eigenvector span the linear space of the smoothing function, which allows us to separate the linear space from the smoothing function. By multiplying the design matrix $X$ and eigenvector matrix $U$ and properly scaling with eigenvalues, we can have a new design matrix such that the smoothing function of variable $x_j$ can be written as
$$
B_j(x_j) = B_j^0(x_j) + B_j^\tp (x_j) = \beta_j X^0_j + \bs {\beta_j^\tp}^T \bs X_j^\tp ,
$$
where $\bs X_j \equiv \begin{bmatrix} X_j^0 : \bs X_j^{\tp} \end{bmatrix}$ as the basis function matrix for the $j$th variable; the coefficients $\bs \beta_j \equiv \begin{bmatrix} \beta_j : \bs \beta^\tp_j \end{bmatrix}$ is an augmentation of the coefficient scalar $\beta_j$ of linear space and the coefficient vector $\bs \beta^\tp_j$ of non-linear space.

- Display the new formulation
