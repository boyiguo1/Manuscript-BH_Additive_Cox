---
bibliography: references.bib
---

\section{Cox Proportional Hazard Additive Model}

For each individual, we collect the the covariate variables $\bs X_i = (X_{i1}, \dots, X_{ip}) \in \mathbb{R}^{p}$ and the survival outcome $(T_i, C_i) \in {\mathbb{R}^+ \times \{0,1\}}$ where $T$ is the observed survival time, and $C$ is a binary variable, marking if the status. When $C=0$, the observation is censored at the survival time $T$ and when $C=1$, we observe the event of interest at time $T$. We assume non-informative right censoring. We also assume there is no competing risk, or multiple occurrence of the event. For the $i$th individual, a tuple of three exists $\{t_i, d_i, \bs x_i\}$, where $\bs x_i$ is a vector of covariates. In addition, the survival time $t_i$ and the censoring indicator $d_i$ are recorded, where $d_i$ takes the value 1 when censoring happens at the survival time $y_i$, and takes the value 0 when the event of interest happens. The Cox proportional hazard model, with additive functions $B_j(x_j) = \beta_jX_j$, is formulated as $$
h(t|\bs x) = h_0(t)exp(a + \sum\limits^p_{j=1}B_j(x_j)) = h_0(t) \exp(a + \sum\limits^p_{j=1}\beta^T_jX_j).
$$ For the purpose of identifiability, we impose an constraint on each additive function $E[B_j(x_j)] = 0$. $h(t)$ is the hazard function and $h_0(t)$ is the baseline hazard function. The hazard function $h(t)$ describes the instantaneous rate of event occurrence among people who are still at risk at the moment. It has a one-to-one relationship with the survival function, which describes the distribution of event time. The linear predictor $\bs x \bs \beta$ is also called the prognostic index. To be noted, different from GLMs, there is no intercept term necessary in the design matrix $\bs x$, as the baseline hazard function $h_0(t)$ estimates an reference level of survival risk. The baseline hazard function indeed changes when additional measure were taken fro the design matrix, such as centering and scaling, while the coefficients $\bs x$ remain unchanged. We defer to \cite{Klein2003, Ibrahim2001} for in-depth survey of the topic.

Unlike the GLM, the estimation for the coefficients $\bs \beta$ doesn't directly maximize log likelihood due to the functional nature of survival analysis. Instead, we consider to maximize the partial log-likelihood function \cite{cox1972}, mathematically, $$
pl(\bs \beta) = \sum\limits^n_{i=1}d_i\log\frac{\exp{\bs x_i \bs \beta}}{\sum_{j\in R(t_i)} \exp(\bs x_j \bs \beta)},
$$ where $R(t_i)$ denotes the risk set at time $t_i$, i.e. the set of all patients who still survived prior to time $t_i$. When tied failure or censoring time exists, a modified partial log-likelihood function can be used \cite{Efron1977}. Conditional on $\beta$, the baseline hazard function $h_0(t_i)$ can be estimated using Breslow estimator \cite{breslow1974}, $$
\hat h_0(t_i|\beta) = d_i/\sum\limits_{i^\prime \in R(t_i)} exp(X_{i^\prime}\beta).
$$

To encourage proper smoothing of the functions, we adopt the idea of smoothing penalties from smoothing spline models \cite{Wood2017}. Smoothing penalty is conventionally formulated as integrated squared second derivative of smooth functions. It's advantages[TODO: two sentences to describe smooth penalty] While it is hard to directly integrate smoothing penalty with sparsity penalty, \cite{Marra2011} proposed a reparameterization to implement smoothing penalty in the design matrix. Given the smoothing penalty matrix $\bs S_j$ is symmetric and positive semi-definite for univariate smoothing functions, we apply eigendecomposition on the penalty matrix $\bs S = \bs U \bs D \bs U^T$ , where eigenvectors and eigenvalues are arranged in the matrices $\bs U$ and $\bs D$ respectively. The zero eigenvalue and its corresponding eigenvector span the linear space of the smoothing function, which allows us to separate the linear space from the smoothing function. By multiplying the design matrix $X$ and eigenvector matrix $U$ and properly scaling with eigenvalues, we can have a new design matrix such that the smoothing function of variable $x_j$ can be written as $$
B_j(x_j) = B_j^0(x_j) + B_j^\tp (x_j) = \beta_j X^0_j + \bs {\beta_j^\tp}^T \bs X_j^\tp ,
$$ where $\bs X_j \equiv \begin{bmatrix} X_j^0 : \bs X_j^{\tp} \end{bmatrix}$ as the basis function matrix for the $j$th variable; the coefficients $\bs \beta_j \equiv \begin{bmatrix} \beta_j : \bs \beta^\tp_j \end{bmatrix}$ is an augmentation of the coefficient scalar $\beta_j$ of linear space and the coefficient vector $\bs \beta^\tp_j$ of non-linear space.

\subsection{Two-part Spike-and-slab LASSO Prior for Smoothing Functions}

To model each additive function, we propose the two-part spike-and-slab LASSO prior for smooth functions under the additive Cox proportional hazard framework. The proposed prior can be considered as an extension of the previous spike-and-slab Lasso prior for group predictors \cite{tang2019}, and has been applied to the GAM settings \cite{guo2022}. The extension focus on

-   separate selection of the linear and nonlinear components of each additive function

-   effective smoothing of the nonlinear effect when exists.

-   Locally adaptive shrinkage on the coefficients. Specifically, the spike-and-slab LASSO prior suppress the weak signal to zero. It provides a natural process for the functional selection in comparison to some other Bayesian regularized prior, for example, spike-and-slab pior, that relies on thresholding.

-   It motivates a scalable model fitting algorithm for high-dimensional data analysis.

To recall, the spike-and-slab LASSO prior \cite{rockova2018a, rockova2018b} is a mixture double exponential prior with a spike density $DE(0, s_0)$ for small effects and a slab density $DE(0, s_1)$ for large effect. Like any other spike and slab priors, the spike is to contain the minimum to zero effects, while the slab is to allow large effects. The scale parameters $s_0$ and $s_1$ are also considered as tuning parameters, which can be optimized via cross-validation. A discussion of how to choose the scale parameters comes later. $s_0$ and $s_1$ are scale parameters, assuming given and $0 < s_0 < s_1$. A latent indicator variable $\gamma_j \in (0,1)$ controls if the the predictor should be include in the model or not. $$
\beta_j|\gamma_j \sim (1-\gamma_j) DE(0, s_0) + \gamma_j DE(0, s_1), 0 < s_0 < s_1.
$$

To accommodate the group structure of the predictors, the group spike-and-slab LASSO prior \cite{tang2018, tang2019} makes the extension by imposing a group specific Bernoulli distribution for the indicator variables,

$$
\gamma_j|\theta_g \sim Bin(1, \theta_g).
$$

The probability parameter $\theta_g$ of group $g$ allows information borrowing across different predictors in the same group, and decides if the group is contributing to outcome. The group-specific hyperprior is built on the premise that if one predictor in the group is included in the model, the rest of the predictors are also more likely to be included in the model. When the group structure doesn't exists, we can treat each predictor to be in their own group, and hence, the spike-and-slab LASSO prior can be treated as a special case of the group spike-and-slab LASSO prior.

In the proposed two-part spike-and-slab LASSO prior for smooth function, we leverage the previous group spike-and-slab LASSO prior and make modification, such as group latent indicator and effect hierarchy principle. Specifically, we impose conditionally independent group SSLs on the linear and nonlinear components of a smoothing function. This is to leverage the fact that the bases of a smooth function have an natural group structure. Given the model matrix of the predictor $X_j$ after the reparameterization step introduce in Section 2.1, we have the linear and nonlinear component of the smoothing function $X_j^0$ and ${\bs X_j}^\tp$. The corresponding coefficients are $\bs \beta_j = [\beta_j:\bs \beta_j^\tp]$. We impose the group SSL priors on the linear and nonlinear components respectively, \begin{align}
  \beta_{j} | \gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \nonumber \\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma_{j}^\tp) s_0 + \gamma_{j}^\tp s_1), k=1,\dots, K_j.
\end{align} To note, we assume that the linear components are one-dimensional and hence using the special case, SSL prior, for simplicity. We make slight modification on the previous group SSL: we force all coefficients of the nonlinear components, i.e. within the same group, to share the same latent binary indicator. This encourages the inclusion of nonlinear components all together and hence, the bi-level selection. Meanwhile, we see that the all the nonlinear components should have the same magnitude of shrinkage after reparameterization, particularly the scaling, which can be considered as the smoothing penalty in the smoohting spline setting.

While the two latent indicators $\gamma_j$ and $\gamma_j^\tp$ controls the inclusion of the linear and nonlinear components of a smooth function, we still need to set up some ordering of the inclusion. For example, it is normally assume that the lower-order effects are more likely to be active than the high-reorder effects (referred to as \textit{effect hierachy} \cite{chipman2006}). In order to implement the effect hierarchy principle in the bi-level selection, we further impose a dependent structure on the latent indicators, \begin{align}
&\gamma_{j} | \theta_j \sim Bin(1, \theta_j) & & 
&\gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j).
\end{align} This is the inclusion of the nonlinear component depends on the inclusion of the linear component. To note, one can easily relax the effect hierarchy by having the two latent indicators be independent condition on the inclusion probability parameter $\theta_j$. The two versions of the indicator prior could introduce trade-off in the variable selection (previously seen in \cite{guo2022}) and will be discussed more in the Section 5. To leverage the fact that the probability of selecting the bases of a smooth function should be similar, we have the linear and nonlinear components (if selected) share the inclusion probability $\theta_j$. It is also possible to have the linear and nonlinear coefficents shares the same indicators, i.e. $\beta_{jk}^\tp$ also depends on $\gamma_j$. However, this approach would disable the bi-level selection ability and force the nonlinear components uses sparsity shrinkage instead of smoothing shrinkage. Hence, it would reduce to a more strict version of group spike-and-slab LASSO where all coefficients in a group employs the same shrinkage instead of locally adaptive shrinkage, and hence it is not recommend here.

The rest of the proposed prior follows the spike-and-slab priors. The inclusion probability parameter $\theta_j$ independently and identically follows a $Beta(a, b)$ distribution. One can consider a special case of the $Beta$ distribution, $Uniform(0,1)$, for simplicity. The prior distribution of the inclusion probability parameter motivates the self-adapative shrinkage for signal sparsity and functional smoothness based on the data. In addition, being a conjugate prior of the binomial distribution, the $Beta$ prior can provide a closed-form solution in the following model fitting algorithm and mitigate some computational burdens. To note, when the variable have large effects in any of the bases, the parameter $\theta_j$ will be estimated large, which in turn encourages the model to include the rest of bases.

<!-- The smoothness of each spline function will be further estimated via the individual $\gamma_jk$. Hereafter, we refer the hierarchical spline Cox Model with the group spike-and-slab mixture normal prior as the ss-Cox Spline Model. -->

\subsection{EM-Coordinate Descent Algorithm for Scalable Model Fitting}

Parsimonious computation is always encouraged in high-dimension data analysis. Many Bayesian methods lose their advantages over penalized models because of their reliance on the computationally prohibitive model fitting algorithms. Previous Bayesian additive models relies heavily on the MCMC algorithm to establish posterior distribution of parameters. One of the exception is \cite{bai2021}, who takes advantages of the optimization-based EM procedure and focuses on the maximum a postiori estimates instead of sampling the posterior distribution. Nevertheless, the algorithm is proposed for the generalized model and could be challenging, if not impossible, to extend to the Cox proportional hazard models.

We develop a fast deterministic algorithm to fit the proposed spike-and-slab LASSO additive Cox model. The algorithm is an extension of the previously proposed EM-coordinate descent algorithm for group spike-and-slab LASSO Cox \cite{tang2019}. Specifically, we first formulate the spike-and-slab LASSO prior as a double exponential prior with a conditional scale prior. Next, we leverage the relationship between posterior density function and penalized likelihood function, $l_1$ penalized specifically, and maximize the posterior density function via coordinate descent algorithm. The feasibility of the optimization process conditions on some nuisance parameters, and hence, we use the Expectation-Maximization procedure to iteratively update the parameters of interests until convergence. We see that similar strategies achieve great computational convenience compared to versions of Monte Carlo Markov Chain algorithms in the high-dimensional data analysis literature, for example \cite{rockova2014, tang2017, guo2022} to name a few.

In the proposed algorithm, our objective is to find the parameters of interests $\Theta = \{\bs \beta, \bs \theta\}$ that maximize the log joint posterior density of $\Theta, \bs gammas$. In Bayesian survival analysis, it is common to approximate posterior density with the product of partial likelihood function in Equation xx and marginal priors. \cite{sinha2003} Hence, our objective function (up to additive constants) is expressed mathematically, \begin{align*}
\max\limits_{\Theta, \bs \gamma} \log f(\bs\beta, \bs \theta, \bs \gamma |t, d) &= \log pl(\bs \beta)+ \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{\tp}_{jk}|\gamma^{\tp}_{j})\right]\| +&\sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{\tp})\log \theta_j + (2-\gamma_j-\gamma_{j}^{\tp}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j).
\end{align*} In case of ties, the partial log-likelihood function can be replaced with Breslow or the Efron approximation [TODO: add citations]. The partial log-likelihood $pl(\beta)$ can also be substitute by the Breslow or the Effron approximation in the presence of ties., and the baseline hazard function $h_0$ is replaced by the Breslow estimator.

Given the latent inclusion indicator is binary and takes only 0 and 1 as its value, a spike-and-slab LASSO prior, as well as the group version, can be expressed as a double exponential prior whose scale parameter is $s_0$ when $\gamma = 0$ and $s_1$ when $\gamma = 1$, \begin{equation}
\beta|\gamma, s_0,s_1 \sim DE(0, (1-\gamma_{j}) s_0 + \gamma_{j} s_1). \nonumber
\end{equation} Leveraging the relationship between double exponential prior and LASSO, the product of partial likelihood function and the prior of $\bs \beta$ can be viewed as an $l_1$ penalized partial likelihood function with penalty $\lambda_j = \{(1-\gamma_{j}) s_0 + \gamma_{j} s_1\}^{-1}$ and $\lambda_j^\tp = \{(1-\gamma_{j}^\tp) s_0 + \gamma_{j}^\tp s_1\}^{-1}$ for $\beta_j$ and $\beta_{jk}$ and optimized with the coordinate descent algorithm [TODO: add citation]. Nevertheless, the optimization requires the knowledge of $\bs \gamma$, which is unknown. To address the problem, we treat the latent indicators $\bs \gamma$ as the "missing data" and use EM algorithm to iteratively derive the MAP estimates. Notably, we establish the expected log joint posterior density of $\Theta, \bs gammas$ with respect to the latent indicators conditioning on the parameters of interest estimated from previous iteration $\Theta^{(t-1)}$.\footnote{The superscription $^{(t)}$ denotes the the parameter estimates at the $t$th iteration.} Hence, we can calculate the equivalent $l_1$ penalty at the $t$-th iteration in the EM algorithm as $\lambda_j^{(t)} = \frac{1-p_{j}^{(t)}}{s_0} + \frac{p_{j}^{(t)}}{s_1}$ and ${\lambda_j^\tp}^{(t)} = \frac{1-{p_{j}^\tp}^{(t)}}{s_0} + \frac{{p_{j}^\tp}^{(t)}}{s_1}$ for $\beta_j$ and $\beta_{jk}$, where $p_{j}^{(t)} \equiv \pr(\gamma_{j}=1|\Theta^{(t-1)})$ and $p_{j}^\tp \equiv \pr(\gamma^\tp_{j}=1|\Theta^{(t-1)})$. To note, for computational convenience, we analytic integrate $\gamma_j$ out of the prior density of $\gamma_j^\tp$. The two quantity $p_j^{(t)}$ and ${p_{j}^\tp}^{(t)}$ can be easily derived with Bayes' theorem and we defer the derivation to \cite{guo2022}. With the expectation set up, we can update $\bs \beta^{(t)}$ with the coordinate descent algorithm as perviously descibed. Meanwhile, conditioning on $\bs \gamma$, the rest of components in Equation (xx) can be optimized independently from the penalized partial likelihood function. It is easy to update $\theta_j^{(t)}$ with a closed-form equation due to the conjugate relationship,
\begin{equation}
\theta_j^{(t)} = \frac{p_j^{(t)} + {p_{j}^\tp}^{(t)} + a - 1 }{a + b}.\nonumber
\end{equation}
The E- and M- steps iterates until meeting the convergence criterion $|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon$, where $d^{(t)} = -2pl(\bs \beta^{(t)})$ and $\epsilon$ is a small value (say $10^{-5}$)

Totally, the proposed EM-CD algorithm is summarized as follows:

1) Choose a starting value $\bs \beta^{(0)}$ and $\bs \theta^{(0)}$ for $\bs \beta$ and $\bs \theta$. For example, we can initialize $\bs \beta^{(0)} = \bs 0$ and $\bs \theta^{(0)} = \bs 0.5$

2) Iterate over the E-step and M-step until convergence

    E-step: calculate $E(\gamma_{j})$, $E(\gamma^\tp_{j})$ and $E({S}^{-1}_{j})$, $E({S^\tp}^{-1}_{j})$ with estimates of $\Theta^{(t-1)}$ from previous iteration

    M-step:

    a) Update $\bs \beta^{(t)}$ by optimizing the penalized likelihood function in Equation (xx) using the coordinate descent algorithm

    b) Update $\bs \theta^{(t)}$ using the closed-form calculation in Equation (\ref{eq:update_theta})

\subsubsection{Selecting Optimal Scale Values}
