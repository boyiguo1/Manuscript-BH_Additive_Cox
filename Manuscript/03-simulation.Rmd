\section{Simulation Studies}

In this section, we evaluate the prediction performance of the propsoed model against three state-of-the-art Cox additive models, including mgcv \cite{Wood2017}, component selection and smoothing operator (COSSO) \cite{leng2006}, and adaptive COSSO \cite{storlie2011}. mgcv is the implementation of generalized additive models with automatic smoothing, and is the one of the most popular method to model nonlinear signals under the Cox proportional hazard framework. To note, mgcv doesn't support analyses when the number of parameters is larger than the sample size, and would not work in the $p>n$ scenario. COSSO and adaptive COSSO is designed to solve the nonlinear effect modelling in the high-dimensional setting. COSSO is one of the earliest additive model that leverage the sparsity-smoothness penalty, and adpative COSSO improves COSSO by using adaptive weight for penalties aiming to relax from the uniform shrinkage applied to all additive functions. The three models of comparison are implemented with R packages \texttt{cosso} 2.1-1 \cite{R_cosso}, and \texttt{mgcv} 1.8-31 \cite{R_mgcv} respectively. To make the evaluation fair, we control multiple implementation factors that could alter the performance, including the smoothing function and tuning of the models. We control the dimensionality of the smoothing functions to 10 bases. We use the most popular cubic spline as the choice of smoothing function for mgcv and the proposed model. COSSO models do not provide any felxibility to define smothing funcitons, and hence use the default choice. We use 5-fold cross-validation to select the tuning parameter among 20 default candidates except \texttt{mgcv} which uses generalized cross-validation to select optimal model. The simulation study is conducted with R 4.1.0 on a high-performance 64-bit Linux platform with 48 cores of 2.70GHz eight-core Intel Xeon E5-2680 processors and 24G of RAM per core.

\subsection{Data Generating Process}

To established a comprehensive understanding of the methods performance, we consider multiple factors that are pivotal to high-dimensional data analysis, nonlinear modeling and survival outcomes. We examine different settings of signal sparsity (defined as the ratio of active variables and total number of covariates), sample size, correlation structure of the predictors, functional form of the underlying signals, and censoring rate.

To describe the data generating process, we generate a total of 1200 data points, where 200 serves as the training data and 1000 serves as the testing data. We consider the number of predictors $p$ to be {4, 10, 50, 100, 200} while limiting the number of active predictors to be 4. We simulate the predictors $\bs X$ from a multivariate normal distribution MVN$_{200\times p}(0, \Sigma)$. The variance covariance matrix $\Sigma$ follows a auto-regressive (AR) structure with two possible order parameters, {0, 0.5}, where $AR(0)$ indicates the predictors are mutually independent. Among the all the predictors, we choose the first four to be the active predictors, i.e. $B_1(x) = (x+1)^2/5, B_2(x) = \exp(x+1)/25, B_3(x_3) = 3*sin(x)/2$, and $B_4(x) = (1.4*x+0.5)/2$. The rest of the predictors are inactive, i.e. $B_j(x) = 0$ for $j = 5, \dots, p.$ To simulate the the survival response, we first generate the "true" survival time $T_i$ for each individual from a Weibull distribution with the scale parameter 1 and shape parameter 1.2 [todo: list the distribution form with linear predictor] with the help of R package \texttt{simsurv} 1.0.0 \cite{R_simsurv}. We then generate the independent censoring time $C_i$ following a Weibull distribution with shape parameter 0.8. We use \cite{wan2016} to estimate the scale parameter so that the censoring rate is controlled at {0.15, 0.3, 0.45}. To note, numeric problems can happen when estimating the scale parameter. Instead, we use the median of other estimated scale parameters of the same simulation setting. The observed censored survival time is the minimum of the "true" survival and censoring time $t_i= min(T_i, C_i)$. The censoring indicator was set to be $I(C_i>T_i)$.

In each iteration of the simulation process, we independently generate the training and testing datasets following the previously described data generation process. We use the training dataset to construct each model of comparison and find the optimal model using 5-fold cross-validation. Then we use the fitted model to make predictions for the testing dataset and calculate the evaluation metrics, including deviance and C-index.

\subsection{Simulation Results}

Across all the simulation settings, the empirical censoring rate is controlled at the desired level (see Table \ref{tab:sim_cnr_prop}). As previously mentioned, mgcv doesn't fit model when the number of parameters exceeds the sample size, and hence ignored in p = {100,200} evaluations. We also experience some programming errors when fitting COSSO and adaptive COSSO models when p is small. The proposed method is robust to programming errors in all examined settings. Our following performance evaluation only use successful iterations. In the following evaluation, we only summarize the performance from success runs.

Overall, we see consistent performance across different settings of dimensional, censoring rate and correlation structure (see Figure \ref{fig:sim_cindex}): the proposed bamlasso model performs as good as, if not better than previous methods, including mgcv, COSSO, adaptive COSSO. The improvement is more substantial as p increases, or higher censoring rate, or when predictors are independent. As expected, adaptive COSSO performs slightly better than COSSO.
