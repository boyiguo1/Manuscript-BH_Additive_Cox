\section{Simulation Studies}
In this section, we evaluate the prediction performance of the propsoed model against three state-of-the-art methods for Cox additive models, mgcv, component selection and smoothing operator (COSSO), and adaptive COSSO. mgcv is the implementation of generalized additive models with automatic smoothing, and is the one of the most popular method to model nonlinear signals under the Cox proportional hazard framework. To note, mgcv doesn't support analyses when the number of parameters is larger than the sample size, and would not work in the $p>n$ scenario. COSSO and adaptive COSSO is designed to solve the nonlinear effect modelling in the high-dimensional setting. COSSO is one of the earliest additive model that leverage the sparsity-smoothness penalty, and adpative COSSO improves COSSO by using adaptive weight for penalties aiming to relax from the uniform shrinkage applied to all additive functions. The simulations are conducted with R 4.1.0 on a high-performance 64-bit Linux platform with 48 cores of 2.70GHz eight-core Intel Xeon E5-2680 processors and 24G of RAM per core. The three models of comparison are implemented with R packages \texttt{cosso} 2.1-1, and \texttt{mgcv} 1.8-31 respectively. To make the evaluation fair, we control multiple implementation factors that would alter the performance, including the smoothing function and tuning of the models. We control the dimensionality of the smoothing functions to 10 bases.  We use the most popular cubic spline as the choice of smoothing function for mgcv and the proposed model. COSSO models do not provide any felxibility to define smothing funcitons, and hence use the default choice [todo: add what it is]. We use 5-fold cross-validation to select the tuning parameter among 20 default candidates.


\subsection{Data Generating Process}
To build a comprehensive understanding of the methods performance, we consider multiple factors that are pivotal of high-dimensional data analysis, nonlinear modeling and survival outcomes. We consider the sparsity of the data defined as the ratio of active variables and total number of covariates, the sample size, correlation structure of the predictors, different functional form of the underlying signals (linear functions, sine function, quadrative function and exponential function), censoring rate.  

To describe the data generating process, we generate a total of 1200 data points, where 200 serves as the training data and 1000 serves as the testing data. We consider the number of predictors $p$ to be 4, 10, 50, 100, 200 while limiting the number of active predictors to be 4. We simulate the perdictors $\bs X$ from a multivariate normal distribution MVN$_{200\timesp}(0, \Sigma)$. We consider the variance covariance matrix $\Sigma$ to be auto-regressive (AR) with two parameter, 0 and 0.5, where $AR(0)$ represents the scenario where the predictors are mutually independent.



focus on our evaluation the following factors that sparsity of the signal, difference choice of additive functions, when designing the simulation study data gernating procees.


<!-- Our data generating process is motivated by Bai [todo: add citation]: -->
<!-- The objective of the simulation studies is to examine the performance of the proposed model under various settings, including different settings of  -->
<!-- - sparsity, i.e. number of active variables (whose effect is not zero across covariate range) / total number of covariates included in the model -->
<!-- - sample size to number of variable ratio,  -->
<!-- - correlation structures among the covariates when for identifiability purpose -->

- mixture of additive functions
  - linear functions
  - sin functions
  - quadrative functions
  - exponential functions
  
Particularly relavent to survival outcomes, in addition, we consider various settings of censoring rate, underlying distributions of baseline hazard function, which include commonly used familyes, Exponential, Weibull, Gumptry

"Cross-validated Harrellâ€™s C-index (Harrell Jr et al. 1982, 1984, 1996) and Brier score (Brier 1950) are used as measures of predictive performance."

While the primary focus of the this simulation studies is to examine the predictive performance of the model, and hence, we select the followings as evaluation metrics:
-
- Caliberation: as we know the $\eta_i$ for the test data, hence, we can compare $\eta_i$ with its estimate $\hat \eta_i$ via mean squared function mean absolute function.
- Discrimination: We use Uno etal's concordiance index. Even though Harrel's concordiance index is more popular, it suffers from bias when censoring presents.[Cite Rahman et al. 2017]

In each iteration of the simulation process, we independently generate the training and testing datasets following the previously described data generation process. We use the training dataset to construct each model of comparison and find the \`optimal\` model using 10-fold cross-validation. Then we use the fitted model to make prediction for the testing dataset and calculate the evaluation metrics.


? How to validate the data generating process works


As some of the evaluation metrics are relative, we compare the performance to a state-of-the-art survival model, which include
- COSSO
- Adaptive COSSO (maybe)
- group penalized models, group bayesian models

\subsection{Simulation Results}

### Algorithm Failures
before delving into the model performance evaluation, we would like to discuss the number of algorithm fails when runing simulations for the compared methods. Supplement Tab xx shows
* mgcv does not work for censarios where the number of coefficients is greater than the number sample size.
* cosso and acosso experienced programming failure particularlay when p is small. Please see appendix for the explaination and bug report.
* the proposed methods programming are robust in all conidtions.

In the following evaluation, we only summarize the performance from success runs.

### Deviance

```{r dvn_tab, results = "asis", eval = FALSE}
sim_test_tab[[1]] %>%
  xtable %>% 
  print(comment = FALSE)
```

```{r dvn_viz, eval = FALSE}
sim_test_viz[[1]]
```

### Cindex


```{r cdx_tab, results = "asis", eval = FALSE}
sim_test_tab[[2]] %>%
  xtable %>% 
  print(comment = FALSE)
```

```{r cdx_viz, eval = FALSE}
sim_test_viz[[2]] +
  ylim(c(0.5, 1))
```

