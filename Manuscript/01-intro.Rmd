\section{Introduction}

Large volume of biomedical data are generated in an exploding speed thanks to the recent advancements in technology. One particularly type of the large volume data is the high-dimensional data, where the number of predictors/dimensions are close if not more than the sample size ($p>>n$), commonly seen in -omics data and high-resolution image data. When the outcome of these data is right-censored time-to-event outcome, Cox proportional hazard model (hereafter refereed to as Cox model) [TODO: add citation] is one of the most popular analytic choices. Cox model describes the change of risk, modeled as hazard, on the multiplicative scale and provides easily interpretable insights in disease etiology.

When a variable enters the Cox model as a predictors, an implicit assumption is imposed: the effect of the predictor are linear. The linear assumption is restrictive and doubtful in many case. A viable solution is to replace each variable with its functional form such that the modified Cox model retains its interpretability while gaining flexibility to model complex signals. The modified Cox model (hereafter referred to as additive Cox model) has many successful applications in biomedical research, for example, dose-response curve modeling, yyy, zzz. To clarify, even both models involve using additive functions to flexibly model signals, additive Cox models are different from additive hazards models [TODO: add Aalan O (1980)], mainly because they answer different scientific questions. Additive Cox models measure hazard change on the multiplicative scale and produce risk ratio interpretation, while additive hazards models measure hazard change on the additive scale and produce risk difference interpretation. In this manuscript, we limit our focus in the additive Cox models, and defer interested readers to \cite{}[Lin and Ying (1997)] for a review of additive hazards models.

[Short transition of additive functions. Among many choices[TODO: add citation xxx] of additive functions, spline function is the most popular choice, and smoothness and mathematically simple, and hence easy to interpret]

Recent years, there are much attention on incorporating splines in survival models, i.e. survival spline models. The topic is greatly discussed in the epidemiology literature in the context of dose-response analysis [@Steenland2004]. The application of spline in the Cox proportional hazard model is not complicated. 
$$
h(t|\bs x) = h_0(t)exp(a + \sum\limits^p_{j=1}B_j(x_j)),\qquad E[B_j(x_j)] = 0.
$$
While the smoothing spline is constructed matrix form, the Cox proportional hazard model can take the smooth spline as the part of the design matrix. Estimation of the coefficients follows the estimation process in the original Cox proportional hazard model. When penalized spline is used, a penalized partial likelihood can be used to estimate. Govindarajulu and her group provided comparison of smoothing methods with Cox models use real world data [@Govindarajulu2007] and synthetic data [@Govindarajulu2009]. To the best of our knowledge, there are not a lot of research done on expanding survival spline model to high-dimensional setting.



The development of expanding Cox models for high-dimensional data analysis includes xxx,yyy,zzz citations.
To extend the additive Cox models to the desired high-dimensional data analysis is non-trivial.

\subsection{Literature review}
-   \cite{Lin2013} reviewed curve interpolation in the context of Cox proportional hazard models, with the aims to provide alternatives to models with implicit linearity with regards to model specification and variable selection. The authors used synthetic data to evaluate the variable selection and predictive performance of the state-of-the-art (by 2013) linear and nonlinear Cox proportional hazard models, i.e. COSSO, adaptive COSSO, LASSO, Adaptive LASSO. Despite the computation intensity of nonlinear (nonparametric) models, the authors suggests to compare the linear models and non-linear models to make assertion on which model is more appropriate.

    In the simulation, the authors examined high-dimensional problem where the author took a two-step strategy: screening + models. This strategy is a little bit surprising to me as the models can be directly applied to address the high dimension problem, even poorly. Overall, the article is very well written, and can be used as **a template for review/ evaluation articles**.

\subsection{Cox Model}

-   \cite{Leng2006} is one among the earliest literature that expands nonparametric Cox models to the high-dimensional setting, where variable selection is of primary interests. The author applied component selection and smoothing operator method (COSSO) in the context of proportional hazard regression. The advantage of COSSO, in comparison to prior works say smoothing spline, is to shrink coefficients to be exactly zero, mirroring between LASSO and ridge regression. A gradient descent algorithm was provided for model fitting. The tuning of the model is achieved by selecting the smoothing parameter with the minimum approximate cross-validation score, a modification of cross-validation criterion. Another advantage of the proposed method is its ability to model effect of two variables as a surface, similar to ANOVA. The simulation considered a synthetic dataset with 8 variables, among which 5 are active (non-zero coefficient). Both correlation structure among predictors, censoring rate and sample size are considered.

-   \cite{Du2010} proposed another additive cox model with the focus on variable selection among parametric covariates, via sparse penalty, and smoothing of non-parametric additive functions, via smoothing penalty. The joint estimation of parametric coefficeints and smoothing function is achieved by an iterative two-step procedure, reciprocal maximization/minimization of likelihood functions conditioning on with assuming the parametric and non-parametric components from previous iteration respectively. Model selection is via a one-step approximation of the SCAD penalty for the parametric part of the model, and minimizing a proposed Kullback--Leibler ratio statistics for nonparametric part. The numerical studies examine the model performance under situations where sample size and censoring rate are the variables of RNG. In discussions, authors gives some directions where some extension of the current framework is possible.

-   \cite{Lian2013} proposed an high-dimensional additive Cox model that uses SCAD penalties to selecting non-parametric component functions. An active-set-type algorithm was proposed for expedited model fitting compared to the classic local quadratic approximation algorithm. Tuning of the model is necessary where three parameters are considered: SCAD constant $c$, the regulation parameter $\lambda$, and variable size constraint $M$. The author suggest that $c=3.7$ following SCAD recommendation, with $M$ to be specified based on assumption on variable size, and $\lambda$ to be tuned and selected be information criteria, which include (AIC, BIC, EBIC). **Extensive** simulations had been done, studying different model selection criteria on model selection, and there is no consensus made. The author argued the potential of extending the model to semi-parametric Cox model, but didn't implement it. The authors also mentioned "*it is not clear whether and how high dimensionality will affect the estimation of the baseline hazard function*", which can be an interesting question to follow up.

-   \cite{Yang2018} proposed the additive cox model specifically for high-dimensional data. Its primary aim is to screening the the predictors of the model. It expended the partial likelihood function using the Taylor's expansion where The first derivative and second derivative of the partial likelihood can be used as smooth and sparse penalties. The smoothing functions in this framework adapts B-spline, but I think it is possible to generalized to other member of the spline family. Sure screening property is proven.


Other methods that shares the same interpretation as Cox model also exists. For example, Bender, Groll, Scheipl (2018) proposed to model survival outcome with piece-wise expoenential mdoels, which relies on transforming the survival outcome and fit a Poisson GLM. Given the well development of generalized additive model, this approach have wider utility, modeling nonlinear effects, and random effects. Challenges still exists when applying in high-deimensional data analysis, and computational efficiency compared to Cox model in most commonly used analytic cases.

[A paragraph to introduce spike-and-slab.]
Natural disadvantage, for example, prohibited coputational of model fitting algorithm, of Bayesian models doesn't discourage the development of bayesian models in Cox model.There are growing traction to use bayesian hierarhical models to address the high-dimensional data analysis as an alternative to the penalized models. The advantages of Bayesian hierarhical models includes 1) posterior distribution inference and hence uncertainty measures, 2) locally adaptive shrinkage based on data, 3) information borrowing, xxx. We saw braod development in high-dimensional generalized additive models, and survival models. Specifically, previous development of group spike-and-slab LASSO model for survival model. This is also the foundation of the proposed method.

The main objectives of this project is to 1) expand current weaponary for non-linear effect modeling of time-to-event outcome via the bayesian hierarchical venue, which has been proved improvement in the realm of generalized linear models  and generalized additive models compared to penalized model. Hence, we hypothesize that using Bayesian hierarhichy prior will improve the prediction performance for Cox additive model. Meanwhile, depending on the way we construct our basis matrix, as well as the prior set up, we can achieve an bi-level selection for the smoothing function. Here, we define the bi-level function as selection of the variable in the model, and selection of the linear versus non-linear effect of the variable. 

To the best of knowledge, there is yet efforts on studying bayesian heiarchical models, particularaly the spike-and-slab lasso prior under the high-dimensional additive model under Cox proportional hazard paradigm. Where there are tremendous amount of literature that explored Bayesian regularized model for modeling continuous or binary outcome in the context of additive models, there are yet any Bayesian efforts to expand to model survival outcomes as far of our knowledge. We are the first to apply spike-and-slab prior on Cox additive model for the purpose of survival prediction and funciton selection in the context of high-dimensional survival outcome modeling. We see a broad range of applications in precision medicine research, particularly genomics data analysis. One of the critism that penalized model receive in the context of additive model is that the sparsity regularization will over smooth the the smoothing function by forceing many, and hence impact on the accuracy of the risk prediction. this phenomenon was previously observed in the context of generalized linear mel.

