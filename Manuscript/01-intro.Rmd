\section{Introduction}

Cox proportional hazards model (Cox model hereafter) is one of the most popular analytic choices to analyze right-censored time-to-event outcome, also commonly known as survival outcome. Cox model describes the change of risk, modeled as hazard, on the multiplicative scale and provides easily interpretable insights in disease etiology. When a variable enters the Cox model as a predictor, an implicit assumption is imposed: the effect of the predictor are linear. The linear assumption is restrictive and doubtful in many case. A viable solution is to replace each variable with its functional form such that the modified Cox model retains its interpretability while gaining flexibility to model complex signals. The Cox model with additive components (hereafter referred to as additive Cox model) has many successful applications in biomedical research, for example, dose-response curve modeling \cite{Steenland2004}, disease progrognostic \cite{gray1992} to name a few. To clarify, the additive Cox model differs from the additive hazards model \cite{aalen1980} even both models leverage additive functions of predictors. The two models answer different scientific questions: the Cox additive model measures hazard change on the multiplicative scale and produces risk ratio interpretation; the additive hazards model measures hazard change on the additive scale and produces risk difference interpretation. In this manuscript, we limit our focus to the additive Cox model, and defer interested readers to \cite{lin1997} for a review of the additive hazards model.

Splines function are one of the most popular choices for additive functions in CAM, because they are mathematically simple, smooth across the range, and hence provide easy interpretation. Mathematically, a spline funciton is a piece-wise polynomial function with continuity conditions imposed on the function. \cite{Wood2017} To apply spline functions in CAM, one can easily replace each predictor with the matrix form of its corresponding spline function which is known. The estimation of coefficients follows the same procedure as fitting an ordinary Cox model. This approach is called regression spline. Nevertheless, if spline functions are overparameterized, i.e. using more than necessary degree of smoothness, regression spline models incline to be overfitted and the estimated functions are very wiggly. To make data-driven decision on the degrees of smoothness, smoothing penalty are applied, resulting the smoothing spline model \cite{reinsch1967}.

Large volumes of biomedical data motivate the development of high-dimensional statistics. Here, we define high-dimensional statistics as the analytic models to address analyses where the number of predictors/dimensions is close, if not more than, the sample size ($p>>n$), commonly seen in -omics data and high-resolution image data. There have been sustaining efforts to extend Cox model to accommodate high-dimensional setting \cite{tibshirani1997, fan2002, zhang2007, wu2012, bradic2011, fan2010}, with a few allowing a small subset of predictors to be modeled nonparametrically\cite{Du2010} (also known semiparametric regression). The semiparametric regression approaches improve the flexibility of high-dimensional Cox models, but the improvement can be limited when the knowledge about predictors' linearity is lacking, for example during the exploratory data analysis of genomics studies. A nonparametric regression approach is highly sought-after for full flexibility and autonomy.

A technical gap exists when attempting to extend the semiparametric regression to the nonparametric regression under the Cox proportional hazard framework for high-dimensional data analysis. Semiparametric regressions assume all additive functions included are necessary and only address the smoothing of these functions. This strategy would fail when applied to nonparametric regressions, as the selection of additive functions are necessary. In other words, under the nonparametric setting, one needs to address the sparsity of additive functions in addition to the smoothness of effective functions. One school of thoughts is to employ group sparse penalty on the coefficients of additive functions: \cite{leng2006} proposed a functional analogue of LASSO penalty;\cite{Lian2013} proposed to uses SCAD penalties to selecting non-parametric component functions;\cite{Yang2018} proposed a feature screening procedure for the additive cox model. These proposals can serve as effective variable selection tools, but are susceptible to inaccurate risk prediction. When constructing model penalty, function smoothness are not considered. Such strategies are not recommended in the high-dimensional generalized additive model literature, as models are prone to wiggle estimations when underlying signal is smooth. \cite{Meier2009} In addition, these globally penalized models ignore the fact that each additive function can have different degrees of smoothness, and hence introduce an over-smoothed solution. \cite{scheipl2013} There are also other approaches provides flexible solutions to model survival outcome in the high-dimensional setting. \cite{bender2018} proposed to model survival outcome with piece-wise exponential models, which is based on the idea of fit a Poisson GLM with transformed survival outcomes. Nevertheless, this method is not computationally efficient and vulnerable to convergence problem due to the numeric calculations. \cite{wu2019} extended the trend filtering model to the survival setting. The solution is a series of step functions, which would not be optimal if underlying functions are assumed to be smooth.

In this article, we introduce the two-part spike-and-slab LASSO (SSL) prior for smooth functions \cite{guo2022} to the high-dimensional additive Cox model literature. Specially, the two-part SSL prior simultaneously addresses signal sparsity and function smoothness by allowing separate adaptive shrinkages on linear and nonlinear components of additive functions. Built upon the premise that better signal estimation produce better outcome prediction, we adopt the smoothing penalty concept via design matrix reparameterization to encourage accurate function estimation. In addition, the two-part SSL prior motivates the bi-level functional selection. Here, we define the bi-level selection as the decision-making on if a variable should be included in the model and if the variable has linear versus nonlinear effect. To the best of our knowledge, we are the first to apply spike-and-slab LASSO prior in the context of high-dimensional additive Cox model for the purposes of simultaneous survival prediction and functional selection. Meanwhile, fitting a Bayesian hierarchical model for high-dimensional data can be computationally intimidating. Hence, we develop a scalable EM-Coordinate Descent algorithm and provide user-friendly implementation in the open-source software environment R\cite{R2021}. The implementation is freely available via \url{https://github.com/boyiguo1/BHAM}. The proposed framework contributes a flexible and efficient solution to high-dimensional molecular and clinical data analysis.
