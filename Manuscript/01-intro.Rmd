\section{Introduction}

With the lower cost of data storage and computation, large volume of data are generated in an exploding speed. One particularly type of the large volume data is the high-dimensional data, where the number of predictors/dimensions are close if not more than the sample size ($p>>n$). The type of data are very common in the realm of biomedical studies, for example genomes data, high-resolution image data. When the outcome of these data is right-censored time-to-event outcome, Cox proportional hazard model (hereafter refereed as Cox model) is one of the the most popular analytic choices. Cox model describes the change of the event risk, modeled as hazard, on the multipicative scale, and provides easily _interpretable_ insights in disease etiology. The development expanding Cox models for high-dimensional data analysis includes xxx,yyy,zzz citations.

When a variable enters the Cox model as a predictors, an implicit assumption is imposed: effects of the predictor are linear. The linear assumption is restrictive and doubtful in many case. [TODO: add hastie quote] A viable solution is to replace each variable with its functional form such that the modified Cox model retains its interpretability while gaining flexibility to model complex signals. The modified Cox model, hereafter referred as additive Cox model, is successful in dose-repsonse curve modeling literature. These efforts include xxx, xxx, xxx. To clarify, additive Cox model is different from additive hazards models. Additive hazards models mdoel the hazard of predictors on the additive scale. As the two modelling framework answering fundamentally different research question, modling of risk ratio versus risk difference, we limit our focus in the additive Cox models, and defer interested readers to [Lin and Ying (1997)] for additive hazards models review.

To extend the additive Cox models to the desired high-dimensional data analysis is non-trivial.

\subsection{Literature review}
-   \cite{Lin2013} reviewed curve interpolation in the context of Cox proportional hazard models, with the aims to provide alternatives to models with implicit linearity with regards to model specification and variable selection. The authors used synthetic data to evaluate the variable selection and predictive performance of the state-of-the-art (by 2013) linear and nonlinear Cox proportional hazard models, i.e. COSSO, adaptive COSSO, LASSO, Adaptive LASSO. Despite the computation intensity of nonlinear (nonparametric) models, the authors suggests to compare the linear models and non-linear models to make assertion on which model is more appropriate.

    In the simulation, the authors examined high-dimensional problem where the author took a two-step strategy: screening + models. This strategy is a little bit surprising to me as the models can be directly applied to address the high dimension problem, even poorly. Overall, the article is very well written, and can be used as **a template for review/ evaluation articles**.

\subsection{Cox Model}

-   \cite{Leng2006} is one among the earliest literature that expands nonparametric Cox models to the high-dimensional setting, where variable selection is of primary interests. The author applied component selection and smoothing operator method (COSSO) in the context of proportional hazard regression. The advantage of COSSO, in comparison to prior works say smoothing spline, is to shrink coefficients to be exactly zero, mirroring between LASSO and ridge regression. A gradient descent algorithm was provided for model fitting. The tuning of the model is achieved by selecting the smoothing parameter with the minimum approximate cross-validation score, a modification of cross-validation criterion. Another advantage of the proposed method is its ability to model effect of two variables as a surface, similar to ANOVA. The simulation considered a synthetic dataset with 8 variables, among which 5 are active (non-zero coefficient). Both correlation structure among predictors, censoring rate and sample size are considered.

-   \cite{Du2010} proposed another additive cox model with the focus on variable selection among parametric covariates, via sparse penalty, and smoothing of non-parametric additive functions, via smoothing penalty. The joint estimation of parametric coefficeints and smoothing function is achieved by an iterative two-step procedure, reciprocal maximization/minimization of likelihood functions conditioning on with assuming the parametric and non-parametric components from previous iteration respectively. Model selection is via a one-step approximation of the SCAD penalty for the parametric part of the model, and minimizing a proposed Kullback--Leibler ratio statistics for nonparametric part. The numerical studies examine the model performance under situations where sample size and censoring rate are the variables of RNG. In discussions, authors gives some directions where some extension of the current framework is possible.

-   \cite{Lian2013} proposed an high-dimensional additive Cox model that uses SCAD penalties to selecting non-parametric component functions. An active-set-type algorithm was proposed for expedited model fitting compared to the classic local quadratic approximation algorithm. Tuning of the model is necessary where three parameters are considered: SCAD constant $c$, the regulation parameter $\lambda$, and variable size constraint $M$. The author suggest that $c=3.7$ following SCAD recommendation, with $M$ to be specified based on assumption on variable size, and $\lambda$ to be tuned and selected be information criteria, which include (AIC, BIC, EBIC). **Extensive** simulations had been done, studying different model selection criteria on model selection, and there is no consensus made. The author argued the potential of extending the model to semi-parametric Cox model, but didn't implement it. The authors also mentioned "*it is not clear whether and how high dimensionality will affect the estimation of the baseline hazard function*", which can be an interesting question to follow up.

-   \cite{Yang2018} proposed the additive cox model specifically for high-dimensional data. Its primary aim is to screening the the predictors of the model. It expended the partial likelihood function using the Taylor's expansion where The first derivative and second derivative of the partial likelihood can be used as smooth and sparse penalties. The smoothing functions in this framework adapts B-spline, but I think it is possible to generalized to other member of the spline family. Sure screening property is proven.


To the best of knowledge, there is yet efforts on studying bayesian heiarchical models, particularaly the spike-and-slab lasso prior under the high-dimensional additive model under Cox proportional hazard paradigm. Where there are tremendous amount of literature that explored Bayesian regularized model for modeling continuous or binary outcome in the context of additive models, there are yet any Bayesian efforts to expand to model survival outcomes as far of our knowledge. We are the first to apply spike-and-slab prior on Cox additive model for the purpose of survival prediction and funciton selection in the context of high-dimensional survival outcome modeling. We see a broad range of applications in precision medicine research, particularly genomics data analysis. One of the critism that penalized model receive in the context of additive model is that the sparsity regularization will over smooth the the smoothing function by forceing many, and hence impact on the accuracy of the risk prediction. this phenomenon was previously observed in the context of generalized linear mel.

 
Significance:
We are the firs to apply the spike-and-slab lasso prior in the cox proportional hazard additive model paradigm


The main objectives of this project is to 1) expand current weaponary for non-linear effect modeling of time-to-event outcome via the bayesian hierarchical venue, which has been proved improvement in the realm of generalized linear models  and generalized additive models compared to penalized model. Hence, we hypothesize that using Bayesian hierarhichy prior will improve the prediction performance for Cox additive model. Meanwhile, depending on the way we construct our basis matrix, as well as the prior set up, we can achieve an bi-level selection for the smoothing function. Here, we define the bi-level function as selection of the variable in the model, and selection of the linear versus non-linear effect of the variable. 

Purpose: improved prediction + Bi-level selection



