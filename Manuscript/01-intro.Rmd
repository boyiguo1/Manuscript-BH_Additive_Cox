# Introduction

Cox propotional hazard (PH) model have a long history and is one of the most popular tools to model time-to-event outcome and predict survival risk. While the classical Cox PH model assumes linear effect of a predictor on the log hazard scale, many extensions has been made to consider more flexible way to model nonlinear effects, and yield great success in the dose-repsonse curve modeling literature. These efforts in clude xxx, xxx, xxx.  The main idea is to replace eahc predictor in the model with a smoothing function. Common choice of the smoothign function include cublic spline, xxx.,xxx.

With the lower cost of data storage and computation, large volume of data are generated in an exploding speed, and wait to be analyzed. One particularly type of large volume data is the high-demension data, where the number of predictors/dimensions are close if not more than the number of the . The type of data are very common in the realm of public health/precision medicine studies, for example genomes data analysis, high-resolution image data analysis. Nevertheless, high-dimensional data creates analytic challenges due to the correaltion structures among predictors as well as model overfitting. Regularize models are proposed to address these challenges. Among which, lasso models are the most popular choice. Tibshrani (1997) first applied lasso model in the Cox model, and since then, there are many other improvement has been made (citation/expand). Meanwhile, the Bayesian models are expanded. Other Bayesian models.[Insert spike-and-slab models]. While there


Risk prediction. Sometimes simplies to a classification function. Look through finer length via survival analysis.

While the survival analysis is based on linearity assumption where each perdictor have linear effect on the predictive scale. There has been efforts to relax the linear assumption, for example xxx xxx xxx. 

Messages:
1. Easily generalized to partial linear structure or semi parametric additive model in the Cox proportional Hazard model context
2. Clearification of the terminology (semiparametric model / Cox additive model versus survival additive model)

-Omics, High-dimensional,

High-dimensional additive mode has been studied under the generalized additive model 


To the best of knowledge, there is yet efforts on studying bayesian heiarchical models, particularaly the spike-and-slab lasso prior under the high-dimensional additive model under Cox proportional hazard paradigm. We ackowledge, there are other additive models for studying survival outcomes. However, those model focus on studying the risk differences instead of risk ratio, and hence have different utilities. 



Where there are tremendous amount of literature that explored Bayesian regularized model for modeling continuous or binary outcome in the context of additive models, there are yet any Bayesian efforts to expand to model survival outcomes as far of our knowledge. We are the first to apply spike-and-slab prior on Cox additive model for the purpose of survival prediction and funciton selection in the context of high-dimensional survival outcome modeling. We see a broad range of applications in precision medicine research, particularly genomics data analysis.

 
Significance:
We are the firs to apply the spike-and-slab lasso prior in the cox proportional hazard additive model paradigm


The main objectives of this project is to 1) expand current weaponary for non-linear effect modeling of time-to-event outcome via the bayesian hierarchical venue, which has been proved improvement in the realm of generalized linear models  and generalized additive models compared to penalized model. Hence, we hypothesize that using Bayesian hierarhichy prior will improve the prediction performance for Cox additive model. Meanwhile, depending on the way we construct our basis matrix, as well as the prior set up, we can achieve an bi-level selection for the smoothing function. Here, we define the bi-level function as selection of the variable in the model, and selection of the linear versus non-linear effect of the variable. 

One of the critism that penalized model receive in the context of additive model is that the sparsity regularization will over smooth the the smoothing function by forceing many, and hence impact on the accuracy of the risk prediction. this phenomenon was previously observed in the context of generalized linear model.


Purpose: improved prediction + Bi-level selection



## Literature review
-   @Lin2013 reviewed curve interpolation in the context of Cox proportional hazard models, with the aims to provide alternatives to models with implicit linearity with regards to model specification and variable selection. The authors used synthetic data to evaluate the variable selection and predictive performance of the state-of-the-art (by 2013) linear and nonlinear Cox proportional hazard models, i.e. COSSO, adaptive COSSO, LASSO, Adaptive LASSO. Despite the computation intensity of nonlinear (nonparametric) models, the authors suggests to compare the linear models and non-linear models to make assertion on which model is more appropriate.

    In the simulation, the authors examined high-dimensional problem where the author took a two-step strategy: screening + models. This strategy is a little bit surprising to me as the models can be directly applied to address the high dimension problem, even poorly. Overall, the article is very well written, and can be used as **a template for review/ evaluation articles**.

## Cox Model

-   @Leng2006 is one among the earliest literature that expands nonparametric Cox models to the high-dimensional setting, where variable selection is of primary interests. The author applied component selection and smoothing operator method (COSSO) in the context of proportional hazard regression. The advantage of COSSO, in comparison to prior works say smoothing spline, is to shrink coefficients to be exactly zero, mirroring between LASSO and ridge regression. A gradient descent algorithm was provided for model fitting. The tuning of the model is achieved by selecting the smoothing parameter with the minimum approximate cross-validation score, a modification of cross-validation criterion. Another advantage of the proposed method is its ability to model effect of two variables as a surface, similar to ANOVA. The simulation considered a synthetic dataset with 8 variables, among which 5 are active (non-zero coefficient). Both correlation structure among predictors, censoring rate and sample size are considered.

-   @Du2010 proposed another additive cox model with the focus on variable selection among parametric covariates, via sparse penalty, and smoothing of non-parametric additive functions, via smoothing penalty. The joint estimation of parametric coefficeints and smoothing function is achieved by an iterative two-step procedure, reciprocal maximization/minimization of likelihood functions conditioning on with assuming the parametric and non-parametric components from previous iteration respectively. Model selection is via a one-step approximation of the SCAD penalty for the parametric part of the model, and minimizing a proposed Kullback--Leibler ratio statistics for nonparametric part. The numerical studies examine the model performance under situations where sample size and censoring rate are the variables of RNG. In discussions, authors gives some directions where some extension of the current framework is possible.

-   @Lian2013 proposed an high-dimensional additive Cox model that uses SCAD penalties to selecting non-parametric component functions. An active-set-type algorithm was proposed for expedited model fitting compared to the classic local quadratic approximation algorithm. Tuning of the model is necessary where three parameters are considered: SCAD constant $c$, the regulation parameter $\lambda$, and variable size constraint $M$. The author suggest that $c=3.7$ following SCAD recommendation, with $M$ to be specified based on assumption on variable size, and $\lambda$ to be tuned and selected be information criteria, which include (AIC, BIC, EBIC). **Extensive** simulations had been done, studying different model selection criteria on model selection, and there is no consensus made. The author argued the potential of extending the model to semi-parametric Cox model, but didn't implement it. The authors also mentioned "*it is not clear whether and how high dimensionality will affect the estimation of the baseline hazard function*", which can be an interesting question to follow up.

-   @Yang2018 proposed the additive cox model specifically for high-dimensional data. Its primary aim is to screening the the predictors of the model. It expended the partial likelihood function using the Taylor's expansion where The first derivative and second derivative of the partial likelihood can be used as smooth and sparse penalties. The smoothing functions in this framework adapts B-spline, but I think it is possible to generalized to other member of the spline family. Sure screening property is proven.


